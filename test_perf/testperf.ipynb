{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "/opt/anaconda3/envs/torch2/lib/python3.11/site-packages/accelerate/utils/imports.py:306: UserWarning: Intel Extension for PyTorch 2.3 needs to work with PyTorch 2.3.*, but PyTorch 2.4.0 is found. Please switch to the matching version and run again.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#-----------------------------------------------------------------------------\n",
    "model_name = \"Qwen/Qwen1.5-0.5B-Chat-AWQ\"\n",
    "device = \"cuda:0\"\n",
    "output_path = \"speed.csv\"\n",
    "methods = [\"baseline\", \"KiVi\", \"LogQuant\", \"PartialLogQuant\"]\n",
    "methods = [\"KiVi\"]\n",
    "n_bit_set = [2]\n",
    "full_precision_lengths = [128]\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.LogQuant import (QuantoLogQuantizedCache, LogQuantizedCacheConfig, \n",
    "                          QuantoStreamingQuantizedCache, StreamingQuantizedCacheConfig, \n",
    "                          QuantoPartialStreamingQuantizedCache, PartialStreamingQuantizedCacheConfig,\n",
    "                          QuantoPartialLogQuantizedCache, PartialLogQuantizedCacheConfig,\n",
    "                          QuantoKiViSinkQuantizedCache, KiViSinkQuantizedCacheConfig)\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "data_type = 'auto'\n",
    "new_token_length = 4096\n",
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"THUDM/LongBench\", \"2wikimqa_e\", split=\"test\")\n",
    "\n",
    "# %%\n",
    "from transformers import Cache, CacheConfig, QuantizedCacheConfig, DynamicCache, QuantizedCache, QuantoQuantizedCache\n",
    "import torch\n",
    "\n",
    "# %%\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Cache, CacheConfig, QuantizedCacheConfig, QuantizedCache\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=data_type,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chat_template(input_text, context):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"fact: {context}\\nquestion: {input_text}\\n\"\n",
    "    }\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache(method, n_bits, dense):\n",
    "    if method == \"KiVi\":\n",
    "        config =  QuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            residual_length=dense,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoQuantizedCache(config)\n",
    "    elif method == \"StreamingQuant\":\n",
    "        config =  StreamingQuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            window_length=(dense - 4)//2,\n",
    "            sink_length=4,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoStreamingQuantizedCache(config)\n",
    "    elif method == \"LogQuant\":\n",
    "        config =  LogQuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            window_length=dense//3,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoLogQuantizedCache(config)\n",
    "    elif method == \"PartialStreamingQuant\":\n",
    "        config =  PartialStreamingQuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            window_length=int((dense - 4)/1.5),\n",
    "            sink_length=4,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoPartialStreamingQuantizedCache(config)\n",
    "    elif method == \"PartialLogQuant\":\n",
    "        config =  PartialLogQuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            window_length=dense//2,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoPartialLogQuantizedCache(config)\n",
    "    elif method == \"KiViSink\":\n",
    "        config =  KiViSinkQuantizedCacheConfig(\n",
    "            backend=\"quanto\",\n",
    "            nbits=n_bits,\n",
    "            window_length=dense,\n",
    "            compute_dtype=data_type,\n",
    "            device=device,\n",
    "        )\n",
    "        cache = QuantoKiViSinkQuantizedCache(config)\n",
    "    else:\n",
    "        cache = None\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_cache(model, qid, cache, batch_size):\n",
    "    chat_template = to_chat_template(ds[qid][\"input\"], ds[qid][\"context\"])\n",
    "    input_ids = tokenizer([chat_template for _ in range(batch_size)], return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "    length = input_ids.shape[1]\n",
    "    output_lens = 0\n",
    "    import time\n",
    "    start = time.time()\n",
    "    model.generation_config.temperature=None\n",
    "    model.generation_config.top_p=None\n",
    "    output = model.generate(input_ids, past_key_values=cache, pad_token_id=tokenizer.eos_token_id, max_new_tokens=new_token_length, do_sample=False)\n",
    "    end = time.time()\n",
    "    for out in output:\n",
    "        output_lens += len(out)\n",
    "\n",
    "    del input_ids\n",
    "    del output\n",
    "    \n",
    "    return length, output_lens, end - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/anaconda3/envs/torch2/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "df = pd.DataFrame(columns=[\"method\", \"n_bit\", \"dense\", \"batch_size\", \"qid\", \"input_len\", \"output_len\", \"time\", \"speed\"])\n",
    "for qid in tqdm.tqdm(range(10)):\n",
    "    for method in methods:\n",
    "        if method == \"baseline\":\n",
    "            batch_sizes = [1, 2, 4, 8, 12, 16]\n",
    "        else:\n",
    "            batch_sizes = [1, 2, 4, 8, 12, 16, 24, 32, 48, 64, 80, 96, 112, 128]\n",
    "        for n_bit in n_bit_set:\n",
    "            for dense in full_precision_lengths:\n",
    "                for batch_size in batch_sizes:\n",
    "                    cache = get_cache(method, n_bit, dense)\n",
    "                    length, output_lens, time = run_with_cache(model, qid, cache, batch_size)\n",
    "                    df = pd.concat([df, pd.DataFrame([[method, n_bit, dense, batch_size, qid, length, output_lens, time, output_lens/time]], columns=df.columns)], ignore_index=True)\n",
    "                    df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
