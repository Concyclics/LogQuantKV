{"pred": "unanswerable.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which was originally proposed for face recognition. It works exactly like NetVLAD, but adds Ghost clusters along with the NetVLAD clusters. Ghost clusters are added along with the NetVLAD clusters, and they work as a way to aggregate local descriptors into global features. The GhostVLAD pooling strategy is used to improve the accuracy of language identification task for Indian languages.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Yes, their model outperforms the state-of-the-art results, achieving an accuracy of 0.718 in the emotion classification task.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Context tweets are proposed as an additional feature for the feature engineering-based models, while bidirectional GRU networks with LTC modules show promising results in the context of Hate and Abusive Speech on Twitter. Context tweets are extracted from the tweet text and used to enhance the accuracy of the feature-based models. The use of context tweets in conjunction with other features, such as word-level features and character-level representations, improves the performance of the models.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at Facebook pages that had a good balance of both news and opinion, as well as those that had a diverse range of topics.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Yes, the proposed task is concept-map-based MDS, which is evaluated through a combination of concept-map creation, reference summarization, and evaluation of the concept map itself. The evaluation is based on a concept-map-based MDS, where the concept map serves as a summary of the document cluster, while the reference summaries are evaluated using TrueSkill, a machine learning algorithm, to compare the concept map with the reference summaries.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "Yes, the proposed approach, GM$\\_$KL, outperforms the previous approaches, including w2g and w2gm, on the benchmark word similarity datasets and entailment datasets. The proposed approach uses a KL divergence-based energy function, which captures both word similarity and entailment, while the other approaches focus on word similarity and entailment separately. Additionally, the proposed approach has a stricter upper and lower bound on KL compared to the other approaches, enabling it to capture more nuanced relationships between words and their contexts.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by averaging the predictions from the constituent single models. The model is first trained on the original CBT training data, and then the best performing model is added to the ensemble if it improves the validation performance. The ensemble is then updated after each iteration, and the process continues until convergence. The model is chosen based on the validation performance, and the final ensemble model achieves high accuracy on the CBT test datasets.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "SocialNLP 2019 EmotionX, the dataset consists of two subsets, Friends and EmotionPush, according to the source of the dialogues.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Yes, the proposed system outperforms strong baseline systems.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes, they did. They presented a new, freely available dataset of simultaneous eye-tracking and EEG data during natural reading and annotation.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Dataset used in the article: Tabular data, which is a set of utterances and their corresponding responses.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of the SMT and various NMT models on our built dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "Three regularization terms are introduced in the article, including neutral features, maximum entropy, and KL divergence between the reference and predicted class distributions. These terms aim to reduce the sensitivity of the prior knowledge and make the model more robust and practical. The neutral features are obtained by selecting the most frequent words as a feature pool, while the maximum entropy and KL divergence terms are obtained by constraining the model to the model distribution and the reference distribution, respectively.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, CNN with word embeddings, and Recurrent Neural Networks with topic models.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Yes, the neural network-based models improved the performance by a significant margin, achieving the best performance with the use of the multitask learning approach.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Yes, our adaptively sparse attention mechanisms, specifically the $\\alpha$-entmax, lead to a higher degree of interpretability compared to softmax transformers, resulting in a more accurate and diverse set of attention weights, which enables the model to better capture different types of linguistic phenomena and individual head behavior, leading to increased interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the monolingual data used for training the DocRepair model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The evaluation metric used in this work is the labeled attachment scores (LAS) for zero-shot cross-lingual transfer. The LAS measures the quality of the contextualized representations learned by the model, including the accuracy of predicting the correct word in a sentence, the percentage of correctly predicted subwords, and the overall accuracy of the model. The results are reported in Table TABREF32. The best LAS scores are obtained for XNLI and dependency parsing tasks, with RAMEN achieving the highest scores. The results suggest that RAMEN+RoBERTa achieves comparable performance to mBERT and outperforms mBERT on several languages, including", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "No. The attention module is not pre-trained on the pre-training stage. Instead, it is shared with the encoder of the ASR model.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Cognitive features derived from human eye-tracking measurements during the process of reading sarcastic texts.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "An LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is a useful source of taxonomic knowledge for taxonomic hierarchy, allowing for the definition and synonymy extraction, which can help evaluate the model's ability to understand and apply definitions in the science domain.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Jasper was built on top of prior work that includes wav2letter, which uses a stack of 1D-convolution and batch normalization. The authors improved wav2letter by increasing the model depth and adding residual connections. Their best configuration was Jasper, which uses a stack of 1D-convolution, batch normalization, ReLU, and dropout.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1, UMA, and MRR.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for the question-answering and answerable classification spans in the dataset.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Yes, the task-specific sentence encoder is learned from scratch for this task.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "four machine translation tasks.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "Yes, the ELMo embeddings improve the performance of Estonian in the NER task.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have experience in the humanities and social sciences, specifically in the areas of computational text analysis, and the challenges and limitations of doing so.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the performance of deep LSTM RNNs with LSTM RNNs, as well as the performance of LSTM RNNs with Xavier initialization and the weight decay method.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Their data set consists of 29,794 articles from Wikipedia, 21,230 arXiv articles, and 2,048 arXiv articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human evaluations were performed to assess the quality of the translation outputs between the RNNMorph and the phrase-based SMT model. The results showed that the RNNMorph model produced better translations, particularly in the area of fluency and accuracy, compared to the phrase-based SMT model. The human evaluation Kappa co-efficient for the RNNMorph model was significantly higher than that of the RNNSearch + Word2Vec model, indicating that the RNNMorph model was more effective in improving the translation quality. The human judgments were based on the translation outputs, specifically the ranking of the RNNMorph model,", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by measuring the efficiency of the communication scheme, including the retention rate of tokens, accuracy of reconstruction, and interpretability of keywords. The effectiveness of the communication scheme is quantified by comparing it to two baselines, Unif and Stopword, which are optimized for different purposes. The constrained optimization objective is formulated as a sequence of constrained optimization problems, where the expected cost and reconstruction loss are minimized simultaneously. The proposed objective is more stable and efficient than the traditional approach, and the optimal value of the objective is found to be more stable as a function of the trade-off parameter $\\lambda$. The optimization is performed using an unbiased", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are evaluated for the multi-class multi-label classification tasks.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "source domain: Amazon benchmark, target domain: Amazon benchmark.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art models, including RAN, QRNN, and NAS, on the PTB and WT-2 datasets.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides a two-layer solution to satisfy the requirements from all three typical engineers. The Block Zoo consists of various end-to-end network templates, blocks, and reusable components, which can be used to construct complex network architectures. The Block Zoo also offers a variety of NLP tasks, including sentence classification, text classification, sequence labeling, knowledge distillation, extractive machine comprehension, and sequence labeling.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the Carnegie Mellon Pronouncing Dictionary BIBREF13 and the multilingual corpus of deri2016grapheme for all languages.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used in the experiments for Speculation Cue Detection and Scope Resolution were BERT, XLNet, and RoBERTa, which were trained on the respective datasets.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "They use English for the Translate-Test and zero-shot approaches, and Spanish or Finnish for the Translate-Train approach.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "tweet2vec outperforms the word-based approach, demonstrating its applicability to generalizing to rare and out-of-vocabulary words.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes. PolyReponse was evaluated against a set of benchmarked conversational search and response retrieval tasks using Reddit data, including the Reddit training data, the Yelp dataset, and the OpenSubtitles dataset. The evaluation metrics included the average response similarity scores, the ranking accuracy, and the overall system performance. The results showed that PolyReponse outperforms existing methods in terms of both accuracy and efficiency, making it a promising solution for building general-purpose task-oriented dialogue systems.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the information found in the dataset to build maps that reflect the geographic lexical variation across the different states, cities, and functions. This allows them to generate maps that demonstrate the distribution of psycholinguistic and semantic word classes, as well as the usage of core values by Levenshtein.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "ML methods aim to identify argument components on the discourse level, which is a discourse-level argumentation.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Yes.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "unanswerable", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Two datasets are used to evaluate the performance of the model: the Wikipedia conversations gone awry dataset BIBREF9, which contains civil-starting Wikipedia talks, and the ChangeView dataset BIBREF20, which represents a subreddit where the aim is to forecast whether a conversation will eventually lead to a decrease in the number of comments.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated through a series of quality checks, including sentence-level BLEU scores, word error rate (WER) and character error rate (CER) using the VizSeq BIBREF17, and the ratio of English characters in the translations. Additionally, the dataset is filtered by sentence length and speaker demographics, resulting in a smaller evaluation set and a more challenging task. The evaluation set is constructed from the Tatoeba corpus, and the BLEU scores are compared to the results obtained from the ASR and MT models.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They propose a novel multimodal approach that combines the information from audio and text data in the speech emotion recognition task by utilizing a dual recurrent encoder (ARE) and a text recurrent encoder (TRE) to simultaneously predict the emotion class of a given signal. The audio features are converted to a subset of the sequential features, and the text features are extracted from the audio signal and converted into a sequence of tokens using the NLTK library. The resulting sequence of embedded tokens is fed into a text recurrent encoder (TRE) and the audio features are converted to a sequence of embedded tokens using the ARE. The final sequence of embedded tokens is concatenated", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "Yes, human evaluation was conducted.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets going viral.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "unanswerable", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "the DeepMine project.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Two deep learning models, specifically a 600d ReLU-based RQE model and a 1,139 question pairs, are used in the construction of the Medical Question Answering (MRA) dataset and the evaluation of the RQE system on the SemEval-cQA dataset. The RQE system outperforms the other methods on the SemEval-cQA dataset, achieving 44.65% accuracy.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Yes, the benchmark dataset is the Weibo dataset, and its quality is high.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "LSTM", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is BERT, which achieves F1 scores of 0.673 on dev (internal) and 0.671 on dev (external).", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline was a weak baseline without any multilingual data, consisting of a single NMT model trained on the in-domain parallel data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nExplanation: In the 7th test batch Set, the system UNCC_QA3 achieved the highest recall score of 0.7033, while the system UNCC_QA1 achieved a score of 48.4% and 12% lower than the highest score of 0.1119 in the 6th test batch.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding methods such as word2vec BIBREF9 are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They pre-order English sentences to match the word order of the source language and train the parent model on this pre-ordered corpus.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Yes, experts with legal training were used for annotation.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "CNN-RNN-based image-to-poem model and seq2seq models with a dictionary of pre-trained word embeddings for language style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "Yes.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the topics of personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation for CNNs.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Yes, the dataset contains six different entity types: Person (PER), Location (LOC), and Organization (ORG).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Yes, the resulting annotated data is higher quality, as evidenced by the higher agreement between expert and lay annotators for the test set, as well as the higher accuracy and precision of the task routing model. This indicates that the annotation quality is improved through the use of difficulty prediction and the incorporation of expert annotations.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "imbalance in analyzed corpora: 61.29% for women on prepared speech and 38.56% for men on spontaneous speech.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Multi30K", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Yes, the article compares the performance of different baselines in terms of CWS in which the model is modelled as a sequence labeling task with character position tags, which is followed by a decoder which performs actual segmentation according to the encoder. The models are classified into three categories roughly: traditional neural models, which use a CRF-based model, semi-CRF-based models, and neural models, which use a Gaussian masker and a bi-affine scorer. The main difference between the traditional and neural models is the way they handle the word representation and the choice of the graph model for CWS task. The model for CWS task is", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multi-layer Perceptron.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLP toolkits such as NLTK, Stanford NLP, TwitterNLP, and BIBREF23, which includes TwitterNLP and BIBREF27.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are conducted on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "existing approaches for modeling geographic locations using Flickr tags, numerical features, and structured datasets include:\n\n  * BIBREF7, which proposes a method for learning word embeddings based on the assumption that the occurrences of tags used in the same aspect of the same document are likely to be similar.\n  * BIBREF44, which introduces a method for learning vector space representations of locations based on the occurrences of tags and the associated features.\n  * BIBREF43, which focuses on learning vector space representations from Flickr tags and structured data, and finds that the contributions of tags to the representation of a given location are influenced by the occurrences", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used 3 datasets for evaluation: CSAT, 20 newsgroups, and Fisher.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, BIBREF1, BIBREF2, and BIBREF3 were evaluated in previous work, starting with sentences with present tense verbs and replacing content words with randomly selected words with the same part-of-speech and inflection, allowing for greater control over the focus verb's selection. The BIBREF1 agreement prediction dataset focused on comparing the probabilities assigned to the correct inflection of the original verb form versus the incorrect inflection, while the BIBREF2 and BIBREF3 experiments considered the bidirectional nature of the BERT model and the manually constructed stimuli.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "In our approach, the invertibility condition is satisfied by ensuring that the Jacobian determinant of the Gaussian emission matrix is either zero or invertible, and the Jacobian determinant of the neural projector is invertible. This ensures that the model is invertible, allowing for exact inference and marginal likelihood computation.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Proposed qualitative annotation schema includes features such as redundancy, lexical entailment, and required reasoning capabilities, as well as a range of other features related to the linguistic complexity, required reasoning and background knowledge, and factual correctness. The schema categorizes the features into different levels of difficulty, including the level of complexity, required reasoning, factual correctness, and the required linguistic features. The schema provides a common annotation schema for MRC gold standard evaluation, which includes features such as the number of words, longest n-grams, and the presence of ambiguous words or phrases. The schema also highlights the need for the presence of additional reasoning and knowledge, such", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "Both datasets are 82,000 sentences with 11.6 million words, and 2,359 sentences split into 2,000 for training and 359 for testing.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Vanilla ST baseline, Pre-training baselines, Encoder-decoder pre-training, Decoder-decoder pre-training, Encoder-pretrained, and Pre-training.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs and deep learning models are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 was used for word embedding.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Yes, personalized models outperform the baseline in BLEU-1 and UMA, generating more diverse and acceptable recipes. The addition of attention mechanisms and fusion layers improves the coherence and step ordering of generated recipes, leading to better step alignment and recipe-level coherence scores. Personalization is reflected in the generated recipe names, which are strong signals for personalization. The generated recipes are also more diverse and acceptable compared to the baseline, indicating a positive impact on recipe quality. The addition of attention mechanisms and fusion layers helps generate high-quality and specific recipes aligned with historical user preferences.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning includes a binary reward for content preservation and a sentiment reward for sentiment preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The model is limited by the lack of a large collection of Shakespearean poems, which is absent from the training dataset. Additionally, the model struggles to generate coherent Shakespearean prose due to the high vocabulary overlap between the source and target languages, which limits its ability to capture the style of Shakespearean poetry. Furthermore, the model's performance is affected by the choice of the style transfer dataset, as the variation in BLEU scores with the source sentence lengths is shown in Figure FIGREF11.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "Yes, the authors compare their models to three benchmark datasets: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Exposure: The number of retweets was significantly higher for tweets containing fake news, as they were retweeted more than 1000 times. The number of favorites was lower, and the number of hashtags used was similar for both types of tweets. The number of friends/followers was slightly lower for tweets containing fake news, and the number of links was higher. The number of mentions was lower for tweets containing fake news, and the number of URLs was higher. The distribution of friends and followers was similar for both types of tweets, and the number of mentions was higher for tweets containing fake news. The distribution of URLs was", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Task (STAN INLINEFORM0) dataset, which consists of 12,128 English hashtags and their associated tweets from the year 2019.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "unanswerable. The article does not mention any accent-specific information regarding the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace. Word subspace represents a low-dimensional linear subspace in a word vector space, which allows for the representation of word vectors, capturing the context of words and preserving the semantic relationships between words. This representation is mathematically defined by the word2vec model, but it requires the frequency of words to capture the context and preserves the semantic meaning, allowing for the selection of appropriate word vectors for each class. The word subspace formulation is a practical and compact way to represent the content of texts, retaining most of the variability of the data. However, it is limited to the term-frequency weighted word subspace, which is", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF) BIBREF14", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The article does not mention the size of the Augmented LibriVox dataset, so the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "Fine-grained sentiment classification was part of the SemEval-2016 \"Sentiment Analysis in Twitter\" task, and the dataset for fine-grained classification is split into three parts: train, development, and development_test. The data for fine-grained classification is considered highly unbalanced and skewed towards the positive sentiment, with only a minority of tweets having a label of one of the negative classes. The dataset for fine-grained classification is split into train, development, and development_test, with train containing tweets and the remaining parts being used for the development and testing phases.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Yes.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets from WordNet, WordNetQA, and DictionaryQA datasets, as well as the synthetic data from the GNU Collaborative International Dictionary of English (DictionaryQA), are subject to quality control, requiring systematic examination of the data and ensuring that the data is not contaminated by systematic biases, and that the synthetic datasets are not too large, as in the case of the WordNetQA dataset.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive, with the best model achieving a micro-average f-score of 0.368.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed in this paper is a combination of two tags: INLINEFORM0 and INLINEFORM1. The tag sequence consists of the tag INLINEFORM0 indicating that the current word is not a pun and the tag INLINEFORM1 indicating that the current word is a pun. If the tag sequence contains a tag sequence with INLINEFORM0, the text contains a pun and the word corresponding to INLINEFORM1 is the pun.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness refers to the ability of a model to make predictions that are consistent with the prior knowledge provided by the model, ensuring that the model is less sensitive to the presence of uninformative features or biased prior knowledge. Regularization terms such as neutral features, entropy-based regularization, and KL-divergence between the reference and predicted class distributions can help reduce the sensitivity of the prior knowledge and make the model more robust. These terms are applied to the objective function of the GE-FL method, which leverages labeled features as prior knowledge to guide the learning process. By incorporating neutral features, the model is forced to prioritize the remaining classes", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "Other sentence embeddings methods are evaluated on the seven STS benchmarks, including the STS benchmark BIBREF10, which contains 6,000 sentence pairs from the three categories: contrasent, news, and forums. Other methods include the siamese and triplet networks, which are used to derive fixed-sized sentence embeddings. Additionally, the siamese network is evaluated on the semantic textual similarity (STS) benchmark and the argumentative similarity (AFS) dataset.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Yes, the proposed DSC loss outperforms the best baseline results by a large margin on CTB5, CTB6, and CTB4, indicating the three losses are not consistently robust in resolving the data imbalance issue. The proposed DSC loss performs as a hard version of F1-score, which enhances the dominance of easy-negative examples and alleviates the dominating effect of hard-negative examples, resulting in improved performance on all of the three datasets.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: ranking questions in Bing's People Also Ask and classification questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Yes, the baseline models were compared against the sentence-level sentence representations generated by the tag-level tree-LSTM and the traditional tree-based models, including the Recursive Neural Networks (RvNN) and the Latent Tree-LSTM (LT-LSTM) with tag embeddings.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "Core relation (chain) for each topic entity selection.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Name-based nearest-neighbor model (NN)", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "Unwarranted inferences are identified through the use of stereotypes and speculations, while biases are detected through analyzing the amount of adjectives used to describe entities in the Flickr30K dataset.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "unanswerable. The article discusses the current state of the art in machine translation and the difficulty of solving the Winograd Schema Challenge, which focuses on the challenge of identifying the referent of a pronoun in a sentence. The article does not mention any specific language(s) explored in the context of the Winograd Schema Challenge.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models: Sentence Encoder, Multilingual RNNs, and Bidirectional LSTM.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with ILP-based summarization algorithms, including Sumy, Sumy, and logistic regression.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "unanswerable", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "unanswerable.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Yes, they experiment with 7 Indian languages.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Yes.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "Yes, the proposed Human Level Attributes (HLAs) model outperforms the baselines, demonstrating a significant improvement in language style recovery from the dialogue data. The Human-Level Attributes (HLA) model is able to accurately predict the language style of a target character without any dialogue, whereas the baselines, such as Kvmemnn, are limited in capturing the context and character-specific language styles. The Human Level Attributes (HLA) model is able to capture the context and character-specific language styles, leading to a better performance in language style recovery.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Yes. Our model gains significant improvements over existing GAN baselines with increased stability and better performance in terms of forward perplexity and Self-BLEU scores.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can detect biases in the process of collecting or annotating datasets, as some datasets contain implicit biases such as slurs and terms related to racial, sexual, and ethnic minorities, leading to higher rates of misclassification and reduced accuracy. The authors also mention that the model struggles to identify subtle nuances in data, such as the presence of implicit slurs and slurs with no explicit slurs, which can contribute to the misclassification of hate speech as offensive. Additionally, the authors highlight the need for a mix of features, including features extracted from the pre-trained BERT model and fine-tuning strategies, to", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, the authors evaluated the performance of the word count baseline, a simple bag-of-words feature, and a two-stage classifier, but found that the best performing baseline was BERT + Unanswerable, which achieved a F1 score of 39.8.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "OurNepali dataset is in standard CoNLL-2003 IO format, which consists of 14 million words from books, web-texts, and news papers. The dataset is divided into six parts: Sentence-Level, Development Set, and Test Set.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Yes, the proposed method improves the F1 score for paraphrase identification by introducing a dynamic weight adjusting strategy, which assigns a different weight to each training example according to its probability, and making the method more attentive to hard-negative examples, resulting in a higher SOTA performance on the tasks of machine reading comprehension and paraphrase identification.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Multitask learning.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " Stimulus-based data was presented to the subjects to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Yes, Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SENS, Pointer-Gen+ARL-SENS, and Pointer-Gen+ARL-ROUGE are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Traditional machine learning models and neural network models are used on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "bi-directional language models", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The proposed method adjusts the weights of training examples based on the Sørensen-Dice coefficient or Tversky index, which allows the model to assign different weights to different classes according to their probabilities. The weights are dynamically changed as training progresses, with a higher weight assigned to easy-negative examples and a decreasing weight to positive examples. This approach helps to emphasize hard-negative examples and reduces the dominance of easy-negative examples, creating a more balanced dataset and improving the model's ability to distinguish between positive and negative examples.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Yes, the proposed strategies achieve improved performance compared to the baseline methods. Specifically, the authors introduce a method to detect bottlenecks and leverage knowledge graphs to improve exploration algorithms for combinatorial action spaces such as text-games. The method involves detecting bottlenecks and constraining the actions used to pass them, resulting in a more efficient exploration of the action space. Additionally, the knowledge graph-based representation of the state space helps to identify promising states and suggests a more optimal policy for exploration. The results show that these methods significantly outperform the baseline methods, including A2C and A2C-chained, in passing the bottleneck of", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a monolingual model and a multilingual model. The monolingual model uses a Bayesian model for the primary role ordering and repetition preferences, while the multilingual model uses word alignments between sentences in parallel corpora to capture cross-lingual semantic role patterns.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard orthographic transcriptions are made, and specific annotation of foreign words, in this case Spanish, is done to identify any potentially non-standard orthographic corrections.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar-based RNN (ScRNN) for word recognition.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "Yes, NCEL achieves the best performance in four out of five datasets, demonstrating its effectiveness in tackling the challenges of entity linking, especially in the “easy\" dataset TAC2010, where local models struggle to handle the high global information, whereas NCEL outperforms them. Additionally, the proposed model outperforms other state-of-the-art approaches, including Local models, iterative models, and global models, showing its robustness to noise and the ability to generalize to unseen data.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the baseline used in the article is the same as the baseline reported by Felice2014a, which is the CoNLL 2014 shared task benchmark.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the clinical notes from the 2010 i2b2/VA dataset.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the decoder to generate each word in the summary one-word at a time, which can help alleviate the problem of repeating phrases and generating incoherent sequences, which can lead to poor quality context-aware word representations and thus affecting the generation of summary contexts. By incorporating the context-aware context encoder in the decoder, the model can generate more complete and consistent context-aware word representations, which can be beneficial for generating more accurate and coherent summary contexts during the decoding process.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The dataset used in the study is the PPDB, which contains noisy phrase pairs.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF and LDA are used to extract features from pathology reports.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 tweets representing 9,300 tweets. Each tweet is annotated with one or more depressive symptoms, for example, postpartum depression, major depressive disorder, and major depressive disorder.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "Yes.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated into Spanish.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "Content-based classifier in conjunction with two feature selection methods.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Baseline for the FLC task is a simple logistic regression model with default parameters, which is a word-level classification task. The baseline for the SLC task is a hand-crafted approach with data augmentation, which involves oversampling the least represented classes.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with a standard approach based on the INLINEFORM4 tagging scheme, which captures the structural constraint of having a maximum of one word tagged with INLINEFORM4 during the testing phase.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Yes, the political bias of different sources is included in the model, as the methodology involves assigning a political bias label to different outlets, which represents the class of articles for each layer of the multi-layer representation.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient-modern Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "UTCNN has three layers.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the Flickr dataset, which contains 70 million photos with coordinates in Europe, all of which were uploaded to Flickr before the end of September 2015.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two datasets: the NUBes-PHI dataset, which is a corpus of real medical records written in Spanish, and the MEDDOCAN 2019 shared task dataset, which is a benchmark dataset for the detection and classification of sensitive information in Spanish clinical narratives.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used simple and complex features derived from a graph structure, including the feature set reported by joshi2015harnessing, which includes unigram features, simple gaze-based features, and gaze features derived from word sequences and the readability and word count of the text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "OKBC.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe are the targets of the tweets in the stance-annotated tweet dataset.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Additional experiments: In this paper, we conduct several experiments on the transformation from non-ironic sentences to ironic sentences in an unsupervised manner. These include: (1) Case study: We present an example of the error analysis and the sentiment accuracy of different models, including the BackTrans, Unpaired, CrossAlign, and CPTG models. (2) Error analysis: We identify errors in the models' outputs and propose solutions to address them. (3) Improvements: We discuss improvements in the model's effectiveness, such as the use of denoising auto-encoder and back-translation, and the introduction of", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention replaces the standard self-attention with a score matrix that includes the local and directional information. The model first computes the scores for each pair of adjacent characters, and then combines the scores to generate the final prediction. The Gaussian-weight matrix represents the local relationships between characters, and the bi-affine attention scorer assigns a probability to each position of characters, making it easier to determine the boundaries of words. The model uses the bi-affine attention scorer to predict the word boundaries, ensuring the segmentation of words correctly.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "social media.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the base models used in the proposed framework, including the pre-trained CNN-SVM and CNN without using any additional features.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "No.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval 2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average of 19.55 tokens on average, with 156.1 sentences per document, and 167,739 words in total.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "text classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Their model is compared to handcrafted rules-based question classification methods, specifically those used in the TREC question classification dataset.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "These versions of ELMo have been trained on larger datasets, specifically larger corpora than the original English ELMo (large) model, which is based on the original English ELMo (large) model.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "64, 16, 20, 22, 24, 25, 64, 16, 30, 64, 128, 256, 512, 1024, 16, 32, 64, 128, 256, 512, 1024, 256, 512", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare their proposed s2s framework to MLP, Eusboost, MWMOTE, and MCMC.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.4325", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on both the Unsupervised POS Induction task and the Unsupervised Dependency Parsing task.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "Yes. The article states that there are several general-purpose deep learning frameworks, such as TensorFlow, PyTorch, and Keras, which offer high-level abstractions and flexibility but require a significant amount of expertise to master the underlying details. This leads to a lack of customization and reuse, making it difficult for engineers to create effective NLP models. The authors propose the development of a generic two-layer solution called the NLP Toolkit, which provides a range of commonly used network templates and customizable blocks for different tasks. This framework allows engineers to focus on building complex architectures rather than dealing with the intricacies of the underlying frameworks. The authors demonstrate", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "Yes, our proposed KB relation detection model achieves the state of the art on both the SimpleQuestions BIBREF2 dataset and the WebQSP BIBREF25 dataset.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
