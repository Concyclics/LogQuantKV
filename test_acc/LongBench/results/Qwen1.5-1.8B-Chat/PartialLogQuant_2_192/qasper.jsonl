{"pred": "unanswerable.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which was introduced for face recognition. It is an extension to the NetVLAD approach, which was used for place recognition. In GhostVLAD, ghost clusters along with the NetVLAD clusters are added, and the vectors belonging to ghost clusters are ignored during the feature aggregation stage. The GhostVLAD-based pooling strategy is used to improve the accuracy of language identification task for Indian languages.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Yes, their model outperforms the state-of-the-art results, achieving an accuracy of 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Context tweets are proposed as an additional feature for the feature engineering-based models, while bidirectional GRU networks with LTC modules show promising results in detecting abusive language. Context tweets are extracted from the tweet text and concatenated with the outputs of the labeled tweets for CNN and RNN models. The LTC module is used to extract the context information, which improves the accuracy of classification.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at Facebook pages that had a balance of domains and stances, as well as a variety of pages with different distributions of emotions.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Yes, the task is described as concept-map-based MDS, where a concept map is created from a set of related documents, and the task is to generate a summary in a concept map format. The evaluation is based on the concept map itself, with a focus on evaluating the importance of concepts and their connections in the concept map, as well as the overall quality of the generated summaries. The evaluation is based on a combination of task-specific metrics, such as the concept map size, relevance of the extracted concepts, and the quality of the generated summaries. The evaluation is also based on the concept map itself, with a focus on the usefulness", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "Yes, the proposed approach, GM$\\_$KL, outperforms the previous approaches, including w2g and w2gm, in terms of performance on the benchmark word similarity datasets and entailment datasets. Specifically, it achieves better correlation compared to w2g and w2gm, and better Spearman correlation compared to w2g and w2gm on the entailment datasets. Additionally, it demonstrates better performance on the crowdsourced dataset BIBREF30, while w2g and w2gm perform poorly.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by averaging the predictions from the constituent single models. These single models are selected based on the validation performance, and the resulting model is called a greedy ensemble. The ensemble is then trained on the BookTest dataset, and the final model is the one with the best validation performance. The ensemble is built by taking the average of the predictions of the constituent models, and the model with the highest average performance is chosen as the final model. The ensemble is trained on the BookTest dataset, and the model's performance is evaluated on the standard Children's Book Test dataset. The ensemble can be improved by adding more training data, as", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "SocialNLP 2019 EmotionX and EmotionPush.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Yes, the proposed system achieves better performance than the strong baseline system.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Dataset: Cognia Interaction norms, which are used in the context of the chatbot, as well as the use of the chatbot, and the use of the chatbot, which is able to respond to the user utterances, and the chatbot, which is able to respond to the user utterances, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of the SMT and NMT models on the proposed ancient-modern Chinese dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "Three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distributions. These terms are applied to reduce the sensitivity of prior knowledge and make the model more robust.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, CNN with word embeddings, and Recurrent Neural Networks with topic models.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Yes, the neural network-based models with the help of the distributed features (nbow+ and nbow+) improved the performance by several points, resulting in the best performance for the neural network-based models.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Yes, our adaptively sparse attention mechanisms, specifically the $\\alpha$-entmax, lead to a higher degree of interpretability compared to softmax transformers, as demonstrated by the increased average attention weight density vectors and the observation of different types of heads with varying degrees of sparsity, such as sparse and dense heads, and the ability to distribute attention weights in a continuous manner. This property enables the model to better capture the underlying linguistic phenomena captured by the heads, resulting in improved interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the monolingual data used for training the DocRepair model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The evaluation metrics used in this study are:\n\n  * Cross-lingual Natural Language Inference (XNLI) test accuracy\n  * Unsupervised Semantic Attachments Scores (LAS)\n  * Supervised Dependency Parsing (LAS) scores\n\nThese metrics are used to assess the effectiveness of the proposed approach, including the ability to adapt from a pre-trained English model to other languages, effectively fine-tune the model for zero-shot transfer, and evaluate the performance of the model in various downstream NLP tasks.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "No. The attention module is not pre-trained on the pre-training stage. Instead, it is used in fine-tuning stage.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Cognitive features derived from human eye-tracking measurements, including two primary features and two additional features, are used to enhance sarcasm detection in text. These features include:\n\n1. Simple Gaze (simple features): These features are calculated by recording the fixation duration of the reader on the text, which is normalized to total word count and are used to measure the amount of time spent on the text. The simple gaze features are computed for both sarcastic and non-sarcastic texts, providing a baseline for sarcasm detection.\n\n2. Complex Gaze (complex features): These features are derived from the eye-tracking data and incorporate both linguistic and", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "An LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is a useful source of taxonomic knowledge for taxonomic hierarchy, allowing for definitions and definitions to be constructed from WordNet, which can help understand the structure of the knowledge contained in the underlying language models, enabling a more comprehensive and consistent understanding of concepts and their relationships.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Jasper models were compared to other baselines, including wav2letter, ResNet, and traditional N-gram NLP models, in order to determine the best configuration for Jasper. The best configuration was determined by experimenting with different combinations of normalization techniques, residual connections, and optimization algorithms. The best setting was found to be batch norm with ReLU, followed by residual connections, and then a combination of batch norm and ReLU. This configuration resulted in the best performance on LibriSpeech and competitive results on other benchmarks.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1, UMA, and MRR.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for the ground truth output, including the type of symptom and attribute, as well as the presence or absence of a specific word in the question.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Yes. Expert annotations are required for the task-specific encoder, as it is trained on the task-specific data.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "four machine translation tasks.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "Yes, the ELMo embeddings improve the performance of Estonian in the NER task. The results are presented in Table TABLEREF21.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have experience in the humanities and social sciences, specifically in the areas of computational text analysis, and the challenges and opportunities of working with data from different sources, including the use of “dual use” methods, and the need for careful consideration of the potential for harm, such as the potential for “dual use” of data, and the importance of considering the potential “dual use” of data, and the need to consider the potential “dual use” of data, and the potential for “dual use” of data, and the need to consider the potential “dual use” of data, and the need", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and are similar to the Afrikaans and English, while the Sotho languages are similar to the Xitsonga and Tshivenda.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the performance of the following LSTM models: Layer-wise training, Block-wise model-updating filter, and Blockwise model-updated filter.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Their data set consists of 29,794 articles from Wikipedia, 21,230 arXiv articles, and 2,048 arXiv articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgement assemblies were gathered by evaluating the translation outputs of the RNNSearch and RNNMorph models for various translations in the test set. The results were analyzed and compared to determine the improvements in translation quality, resulting in a 41% increase in the 4-gram, 3-gram, and 2-gram BLEU scores for the RNNSearch model, indicating a significant improvement in translation quality. The human evaluation also showed a 51% improvement in the 4-gram BLEU scores for the RNNSearch model, demonstrating the effectiveness of the model in improving translation quality. The human evaluation", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by measuring the efficiency of a communication scheme, which is the number of keywords retained in the keywords-sentence pair, and the accuracy of the scheme, which is the fraction of sentences generated by the system that are semantically equivalent to the target sentence. The effectiveness of the constrained objective is quantified by comparing the achievable accuracy and efficiency tradeoffs for the objective, which is the sum of the costs and losses. The constrained objective is computed by minimizing the expected cost under a reconstruction loss constraint, and the unconstrained objective is computed by maximizing the expected cost subject to a reconstruction loss constraint. The optimization is performed using a single sample", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are evaluated for the multi-class multi-label classification task.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "source domain: Amazon benchmark, target domain: Amazon benchmark.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art methods, including RAN, QRNN, and NAS, on the PTB and WT-2 datasets.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides three main types of neural network modules: Embedding Layer, Neural Network Layers, and Regularized Layers. The Embedding Layer supports both word/character embedding and additional handcrafted features for enhanced performance. The Neural Network Layers provide a variety of blocks, including RNN, CNN, and Transformer blocks, along with attention mechanisms. The regularized layers, such as Dropout, Layer Normalization, and Batch Normalization, are also included for improving the generalization ability of the models. Additionally, the Block Zoo provides a gallery of reusable blocks for constructing complex network architectures, allowing for customization and reuse of components.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the Carnegie Mellon Pronouncing Dictionary BIBREF13 and the multilingual corpus of orthographic and phonemic data from deri2016grapheme.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used in the experiments for Speculation Cue Detection and Scope Resolution were the baseline models BERT, XLNet, and RoBERTa, which were trained on the same datasets as the experimental setup.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "They use English, Spanish, and French in their experiment.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "tweet2vec outperforms the word-based approach, demonstrating its applicability to various NLP tasks such as named entity recognition (NER), POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes. PolyReponse was evaluated against a set of benchmarked conversational search and response retrieval tasks using Reddit data, including a general domain (Reddit), a structured dataset (Yelp), and a large-scale dataset (OpenSubtitles). The evaluation results showed that PolyReponse outperforms existing methods in terms of both accuracy and efficiency in retrieving relevant responses.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the method of generating maps that reflect the geographic lexical variation across the different states, which reflects the core values of the participants.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "ML methods aim to identify argument components on the argumentation in the context of the gold-standard argumentation theory, such as the Toulmin's model, which is a well-established discourse theory.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Yes.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "unanswerable", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes, the 12 languages.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Two datasets are used to evaluate the effectiveness of the proposed model for forecasting derailment of online conversations, which is a task that involves predicting the future derailment of a conversation based on early warning signs. These datasets consist of two datasets: one is an expanded version of the annotated Wikipedia conversations dataset, which contains a series of controlled conversations with clear-cut classifications of whether the conversation eventually leads to personal attacks or remains civil, and the other is a dataset constructed from the Reddit forum ChangeView, where the model aims to predict whether a conversation will eventually receive moderator intervention due to \"rude or hostile\" behavior.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes. The article mentions that the Portuguese NLP modules were trained using a Portuguese model, which is a deep learning model. Additionally, the SRL module was trained on a Portuguese model, which is also a deep learning model. Therefore, the pipeline components mentioned in the article were based on deep learning models.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated through several quality checks, including sentence-level BLEU scores, word error rate (WER) and character error rate (CER) using the VizSeq BIBREF17, and the ratio of English characters in the translations. Additionally, the dataset is checked for overlapping samples, and the evaluation set is constructed from the Tatoeba corpus. The evaluation set is further filtered by sentence length and number of speakers and accents, making it closer to real-world scenarios and more challenging.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They propose a novel multimodal approach that combines the information from audio and text data in the audio and text modalities separately, utilizing a dual recurrent encoder (ARE) and a multilingual text-to-text encoder (TRE) to simultaneously predict the emotion class of a given signal. The ARE model encodes audio and text data independently, while the TRE model uses the same architecture but focuses on the audio signal. The proposed MDRE model combines the advantages of both models, utilizing the audio information from the ARE and incorporating the text information from the TRE. The model uses a novel attention mechanism to selectively attend to the parts of the transcript that contain", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets going viral refers to tweets that went viral within a certain time frame, specifically, the day of the election of Donald Trump as the 45th president of the United States of America.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "unanswerable", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "the DeepMine project BIBREF4.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Two deep learning models, specifically a 600d ReLU-based DL model and a 1,139d LSTM model, are used in the context of the medical domain for the RQE task.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Yes, the benchmark dataset is the Social Honeypot dataset, and its quality is high.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "LSTM", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is BERT, which achieves F1 scores of 0.673 on both SLC and FLC tasks.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline was a weak baseline without any multilingual data, which consisted of a single NMT model trained on the in-domain parallel data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.4325", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding methods such as word2vec BIBREF9 are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use pre-ordering to match words before reordering them.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Yes. The experts used for annotation were legal experts with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "CNN-RNN-based image-to-poem model and seq2seq models with a dictionary of pre-trained word embeddings for text style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "Yes.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the topics of personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation called \"Extended Middle Context\" that combines the left context, the entity in the relation argument, and the middle context. This context is split into two parts: a combination of the left context, the entity, and the middle context, and a combination of the middle context, the right entity, and the right context. Both contexts are processed by two separate convolutional and maxpooling layers, and the results are concatenated to form the sentence representation.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Yes", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Yes, the resulting annotated data is higher quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "imbalance in analyzed corpora is 33.16%, indicating a disparity in gender representation and speech quantity in French media.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Multi30K BIBREF21", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Yes, the article compares the performance of different baselines in terms of CWS in the closed test setting. The models are BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, and BIBREF10, which are considered as strong baselines for CWS modeling. These models are typically designed with a specific architecture and feature representation, making them suitable for CWS tasks. However, the article does not specify which specific model is compared to others, so the answer", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multi-layer Perceptron.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLTK, Stanford CoreNLP, TwitterNLP, and BIBREF17, BIBREF18, and BIBREF19 for entity-level sentiment analysis.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are conducted on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "existing approaches for modeling geographic locations include bag-of-words representations, term-based representations, and vector space embeddings. The goal of using these methods is to represent locations in a natural way, integrating the textual information from Flickr tags and the available structured information from the dataset. The bag-of-words representation is a simple way to encode locations, while term-based representations capture the relevance of tags to the locations. Vector space embeddings, on the other hand, are used to learn representations of locations in a more general manner, allowing for more flexible and effective representation of locations. In this study, the proposed model combines both bag-of-words representations and vector", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used 3 datasets for evaluation: CSAT, 20 newsgroups, and Fisher Phase 1 corpus.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, BIBREF1, BIBREF2, and BIBREF3 were evaluated in previous work, specifically in the context of subject-verb agreement, \"colorless green ideas\" settings, and manually constructed stimuli, respectively.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "In our approach, the invertibility condition is to ensure that the Jacobian determinant of the Jacobian matrix of the non-linear function is either one or equals to zero, indicating that the projection is invertible. This ensures that the information is preserved and prevents information loss due to the inversion.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Proposed qualitative annotation schema includes features such as redundancy, lexical entailment, factual correctness, complexity, semantic complexity, required reasoning, linguistic complexity, required knowledge, lexical overlap, and other relevant features. The annotation schema is organized into categories such as linguistic features, required reasoning, and factual correctness, and it provides a common framework for evaluating MRC gold standards. The annotation schema is based on the annotation of 50 MRC datasets, with 6 gold standard datasets representing different aspects of the task, such as the answer selection style, required reasoning, and factual correctness. The schema categorizes the data based on the features that are marked", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "Both datasets, WikiSmall and WikiLarge, consist of 82,000 sentences with 11.6 million words each.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Vanilla ST baseline, Pre-training baselines, Encoder-decoder pre-training, Decoder-decoder pre-training, Encoder-pretrained, and Pre-training.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs and neural networks are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 was used for word embedding.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Yes, personalized models outperform baselines in BPE perplexity and UMA, achieving better step ordering and local coherence. Personalization also leads to higher UMA and MRR, with the addition of attention mechanisms improving the semantic plausibility of generated recipes. Personalization is reflected in the generated recipes' coherence scores, which range from 1.78-1.82, and the ability to generate recipes that follow a logical and coherent step ordering. Additionally, the generated recipes are more coherent and acceptable compared to baseline, with a mean recipe-level coherence score of 1.82-1.88.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning includes a reward for content preservation, which is defined as the difference between the sentiment polarity of the input sentence and the sentiment of the generated sentence, and a reward for irony accuracy, which is the absolute value of the difference between the sentiment score of the input sentence and the sentiment score of the generated sentence.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the model may not work well with Shakespearean style transfer as shown in Figure 12, due to the lack of a large collection of Shakespearean prose which describes paintings. They suggest expanding the style transfer dataset to improve the performance of the model.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "Yes, the authors compare their models to three existing benchmarks: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Exposure, characterization, and polarization.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from two datasets: (a) the STAN INLINEFORM0 dataset, which consists of 1,108 unique hashtags from the Stanford Sentiment Analysis 2017 dataset, and (b) the STAN INLINEFORM1 dataset, which contains all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis 2017 dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace represents a low-dimensional linear subspace in a word vector space, which retains the majority of the variability of word embeddings, allowing for the separation of words based on their context and preserving the semantic relationships between words. Word subspace can be used to model the word vectors of each class in a text classification task, but it requires a significant amount of data and computational resources to handle the dense and sparse representations of word vectors. The word subspace formulation is a practical and scalable way to represent the content of texts, but it neglects the word semantics, which can be captured by the term-frequency and term-frequency inverse document-frequency weights", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF) BIBREF17.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The article mentions that the post2013 improved introduced a 38-hour Spanish-English ST corpus by augmenting the transcripts of the Fisher and Callhome corpora with English translations. However, the exact number of hours of the Augmented LibriSpeech dataset is not mentioned in the article. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "Fine-grained sentiment classification was part of the SemEval-2016 \"Sentiment Analysis in Twitter\" task, and the dataset for fine-grained classification is split into three parts: train, development, and development_test. The fine-grained sentiment classification is considered harder due to the imbalance in the data and the difficulty of obtaining distant supervision, making it challenging to obtain balanced training data. Therefore, the authors used the high-quality datasets provided by the challenge organizers, including the fine-grained and ternary datasets, and used the data for fine-grained sentiment classification.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Yes.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets from WordNet, WordNetQA, and DictionaryQA, are not subject to quality control, as they are not systematically constructed from a large and diverse set of expert knowledge, the choice of base models, and the use of synthetic data, the choice of question templates, and distractor choices, may not capture the full range of knowledge and reasoning required for a model to be competent in the science domain, as demonstrated by the results in the paper. Additionally, the choice of base models and datasets, as well as the choice of question templates and distractor choices, can affect the quality of the generated datasets,", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive, with the best model achieving a micro-average f-score of 0.368.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed in this work is a combination of two tags: INLINEFORM0 and INLINEFORM1. The tag sequence consists of the tag INLINEFORM0 indicating that the current word is not a pun and the tag INLINEFORM1 indicating that the current word is a pun. If the tag sequence contains a tag sequence that contains a tag INLINEFORM0, the text contains a pun and the word corresponding to INLINEFORM1 is the pun.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness refers to the ability of a model to reduce the sensitivity to prior knowledge, which is typically represented by the number of labeled features and the KL divergence between the reference distribution and the model's predicted distribution. By incorporating the labeled features and KL divergence regularization terms, the model can be made more robust and capable of handling unbalanced labeled features and class distributions, leading to better performance. Regularizing the KL divergence further allows for the incorporation of additional knowledge, making the model more interpretable and less biased towards the desired class distribution. Overall, the proposed approach demonstrates the effectiveness of using prior knowledge to enhance the robustness of a model,", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "Other sentence embeddings methods are evaluated, including the siamese and triplet networks, which are used in the argumentative setting. These methods are compared to cosine-similarity and the maximum output of the sentence embeddings. The siamese network architecture is used to derive fixed-sized sentence embeddings, while the triplet network is used to find the most similar sentence pairs in a collection of 10,000 sentences. The authors evaluate the performance of these methods on the STS benchmark and the AFS dataset, achieving an improvement of 11.7 points on the STS benchmark and 5.5 on the AFS dataset", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Yes, the proposed DSC loss outperforms the best baseline results by a large margin on CTB5, CTB6, and CTB4, indicating that the proposed DSC loss is robust to the data imbalance issue in NER tasks. The proposed DSC loss outperforms BERT-tagger by +1.8 on CTB5, +1.8 on CTB6, and +2.19 on CD on CTB4, +0.29 on CD5, and +0.96 on CD6. The proposed DSC loss achieves SOTA performance on all of the three", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: ranking questions in Bing's People Also Ask and classification questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Yes, they compared their model with other baseline models, including RvNNs, trees-based models, and Gumbel Tree-LSTM, as well as the recently proposed latent tree-structured models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "Core relation (chain) for each topic entity selection.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Name-based nearest-neighbor model (NN)", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "Unwarranted inferences are identified in the Flickr30K dataset, including stereotypes-driven descriptions, which involve excessive use of adjectives to describe individuals who do not conform to traditional gender roles or exhibit unusual behaviors. Other methods include analyzing the use of adjectives and identifying patterns related to old people, ethnicity, and age groups. Detecting bias in language requires manual inspection of descriptions and tagging of phrases with part-of-speech information. A coreference graph can be created to identify referring expressions, and a coreference-based approach can help identify the richness of the data and the potential for bias.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "unanswerable. The article discusses the Winograd Schema Challenge, which is a competition for machine translation programs to identify the correct referent for an ambiguous pronoun in a sentence. The article does not mention any specific language in which the challenge is explored.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n- Models that use not only hidden states but also cell states from the previous layer: Baseline (plain stacked LSTM)\n- Models with different INLINEFORM0: (i) models that do not include INLINEFORM0, and (ii) models that incorporate INLINEFORM0\n- Models that integrate lower contexts via peephole connections: (iii) models that remove INLINEFORM1 and use only INLINEFORM2, and (iv) models that combine lower contexts via peephole connections\nNote that the proposed architecture (CAS-LSTM) is not included in the list of models tested.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with ILP-based summarization algorithms, including Sumy, Sumy, and Sumy, which require a number of sentences kept in the final summary and a suitable value of INLINEFORM0.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "unanswerable", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "unanswerable.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Yes, they experiment with 7 Indian languages.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Yes.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "Yes, the proposed Human Level Attributes (HLA) model outperforms the baselines, achieving a performance difference of around 40.67% in Hits@n/N, demonstrating the effectiveness of the proposed HLA model in generating the language style of specific characters. Additionally, the HLA-OG guidance helps in improving the performance of the CSM component, leading to a higher accuracy of 25.08% in the CSM evaluation. The HLA-OLRAM model outperforms the baselines, achieving a mean score of 40.67% in the HLA-OG evaluation", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Yes. Our model gains significant improvements over the baselines, especially in terms of self-bleu, through the addition of reward augmentation and the use of a stationary distribution in the reward function. The stability of the training process is ensured by the constraint of the reward function, which limits the search space and prevents the generator from being trapped in the vicinity of the real data, making the generator more stable and capable of generating diverse text samples.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can detect biases in the process of collecting or annotating datasets, specifically in the case of Waseem and Hovey's dataset, where the model mislabels tweets as racist, sexist, nor neither, and vice versa, indicating that the model does not capture the subtle nuances of hate speech and its underlying context. Additionally, the authors mention that the model struggles to identify implicit abuses in the dataset, leading to biased results. This suggests that the model may not be able to capture the full range of biases in the data, making it necessary to investigate the impact of different fine-tuning strategies on the", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "OurNepali dataset is in standard CoNLL-2003 IO format, consisting of 14 million words from books, web-texts, and news papers, and 16225 unique words extracted from the POS-tagged Nepali National Corpus.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Yes, the proposed method improves the F1 score for paraphrase identification by introducing a hard version of F1 score, which effectively addresses the dominating effect of easy-negative examples on the task. The proposed DSC loss outperforms other losses like dice loss and focal loss, demonstrating its ability to tackle the data imbalance issue in NLP tasks.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Multitask learning with eye-tracking, self-paced reading time, and ERP data is used.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " Stimulus-based EEG data was presented to the subjects to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Yes, Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+RL, and Pointer-Gen+RL-ROUGE are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Traditional machine learning models, including Word-Level, Character-Level, and Hybrid CNN, are used for feature engineering. Additionally, RNN-based models, such as Bidirectional GRU, are investigated for their ability to handle context tweets. Ensemble models, such as GBT and RF, are also explored for improved performance.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "bi-directional language models", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The proposed method dynamically adjusts weights of training examples according to their corresponding probabilities, with a weight proportional to the difference between the probabilities of positive and negative examples. This approach addresses the dominating effect of easy-negative examples and ensures that the model pays more attention to hard-negative examples, resulting in improved performance on both easy-negative examples and hard-negative examples. The method employs a weight adjustment strategy that assigns different classes to different weights, with the goal of minimizing the dominance of easy-negative examples and promoting the learning of hard-negative examples. The weights are determined by the inverse class frequency or the weight of the Tversky index, which balances precision and recall and", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Yes, the proposed strategies, including the ones based on the knowledge graph, significantly outperform the baseline methods in solving the bottlenecks in the state-action space of Zork1. Specifically, the knowledge graph-based methods achieve a score of around 40, whereas the other methods get stuck in bottlenecks and do not surpass them. The improved performance of these methods is attributed to the ability of the knowledge graph to guide the agent towards the next optimal state, which is more likely to lead to a higher reward. Additionally, the knowledge graph-based methods leverage the knowledge graph to enhance the exploration process, allowing the agent to traverse through", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a monolingual model and a multilingual model. The monolingual model uses a Bayesian model for the primary role ordering and repetition preferences, while the multilingual model uses word alignments between sentences in parallel corpora to capture cross-lingual role correspondences.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard orthographic transcriptions are identified by analyzing the presence of disambiguated words, such as foreign words, in the text. These words are marked with labels that indicate the intended meaning, but may contain non-standard orthographic variations, such as misspellings, which are considered to be valuable for understanding the context and improving the accuracy of speech recognition and machine translation systems.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar-based RNN (ScRNN) is a type of recurrent neural network (RNN) for word recognition, specifically designed to handle misspelled words. The model processes a sentence of words with misspelled characters, treating the first and last characters independently, and the internal characters as a bag of characters. The input representation is obtained by concatenating the internal characters with the external characters, and the word recognition model is trained on a smaller corpus. The word recognition model is trained on a specific domain, and the word error rate (WER) is measured on a test set. The word error rate is a measure", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages are explored.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "Yes, NCEL achieves significant improvements in efficiency and generalization compared to other existing approaches, achieving promising performance in various datasets.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the baseline used in the article is the same as the one reported by Felice2014a, which is the approach used to train error detection systems.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the clinical notes from the 2010 i2b2/VA dataset.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the decoder to generate sequences with a complete context, which helps alleviate the problem of incomplete and inconsistent context in the previous abstractive methods. By masking words in less attended sentences, the decoder can generate more consistent and complete context-aware word representations, which can lead to better generation of summary contexts and thus improve the quality of generated summaries. Additionally, by feeding the summary draft with both context representations, the model can leverage the pre-trained contextual embeddings in BERT, which can help the decoder generate more coherent and fluent sequences, reducing the exposure bias problem. Therefore, masking words in the decoder", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article mentions that the SDAE model uses a Twitter dataset, specifically the Twitter textual entailment task. Therefore, the answer is \"yes\".", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF and LDA are used to extract and encode the text content of pathology reports into structured data. TF-IDF is a term frequency-inverse document frequency measure that assigns higher weights to terms that appear frequently in a document while ignoring terms that appear infrequently. LDA is a Latent Dirichlet Allocation algorithm that groups similar words into topics based on their TF-IDF weights, enabling identification of important keywords within the reports.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 tweets representing 9,300 tweets. Each tweet is annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "Yes.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated to Spanish.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "Content-based classifier in conjunction with two feature selection techniques.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Baseline for the FLC task is a simple logistic regression model with default parameters, where a word can belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. This baseline system demonstrates the inadequacy of a straightforward approach to the task.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with the baseline of a rule-based system that scores candidate words according to eleven simple heuristics, including the phonetic distance via the CMU Pronouncing Dictionary, the state-of-the-art system for homographic pun location, and the state-of-the-art system for pun detection and location.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Yes. The methodology includes a set of features which reflect the presence of different sources, including political bias, in the classification task.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient-modern Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "UTCNN has three layers: a convolutional layer, a max pooling layer, and a fully connected layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF7 , which focuses on predicting climate-related features, such as temperature and land cover types, from a set of locations.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a Spanish clinical narrative dataset that contains real-world medical documents, while MEDDOCAN is a shared task dataset that focuses on the detection and classification of sensitive information in Spanish clinical narratives.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used simple and complex gaze-based features, including unigrams and pragmatic features, as well as two additional linguistic features: readability and word count, to enhance sarcasm detection in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "OKBC is the basis for open-world knowledge learning and inference in conversations, making it a key component of the generic and effective chatbot learning mechanism.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe are the targets of the tweets in the stance-annotated tweet dataset.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Additional experiments are conducted, including error analysis and investigation of improvements in the model's performance. These experiments include conducting automatic evaluations and human evaluations, analyzing the quality of generated sentences, and investigating the effectiveness of different models in transforming irony sentences to non-ironic sentences while preserving content and sentiment polarity. Additionally, the study investigates the difficulty of ironies in different categories and proposes a specific approach to transfer irony from ironic sentences to non-ironic sentences in an unsupervised manner. The proposed method combines style transfer and reinforcement learning to control the content and sentiment polarity of the input sentence, ensuring that the model generates ironic sentences while preserving the content", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of the attention mechanism in Transformer BIBREF24 which replaces the standard self-attention layer with a bi-affine attention scorer to capture the local and directional information. The bi-affine attention scorer assigns a probability to each position of characters, which is influenced by the local and directional information. The bi-affine attention is achieved by adding a bias term to the self-attention layer, which ensures that the relationship between two characters with distant characters is weaker than adjacent characters. The bi-affine attention is applied in a greedy decoding manner to find the boundaries of words, ensuring the segmentation", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "social media.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the base models used in the proposed framework, including the pre-trained CNN-SVM model and the difference-based CNN model.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "No.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were always the same, ranging between 0.716 and 0.718 for the five subtasks.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which are approximately 19,550 tokens in total.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "text classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Their model is compared to handcrafted rules-based question classification methods.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "These versions of ELMo have a significantly larger training set than the previous ones, with 20 million tokens in total, compared to 20 million tokens in the previous ELMo models.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "64, 16, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, and MWMOTE.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.4325", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "Yes. The article states that there are high-level abstractions in the popular deep learning frameworks, such as TensorFlow, PyTorch, and Keras, which make it difficult for engineers to master these frameworks and focus on building custom models. This leads to a lack of flexibility in the development process, making it challenging to create efficient and customizable models for NLP tasks. The article suggests that the use of higher-level abstractions and standard components in the Block Zoo can help alleviate these challenges and enable engineers to build complex models with ease.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "Yes, our proposed KBQA system achieves the state of the art on both SimpleQuestions and WebQSP benchmarks. Specifically, our system outperforms the previous state-of-the-art methods on both benchmarks, achieving 91.2% and 88.9% on SimpleQuestions and 80.65% on WebQSP, respectively. Additionally, our system outperforms the state-of-the-art on WebQSP by 2-3% and 2, showing that our KB relation detection model is able to capture the global information and handle long-range relations. Furthermore, our system outperforms the state", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "unanswerable.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which was introduced for face recognition. It is an extension to the NetVLAD approach, which was used for place recognition. In GhostVLAD, ghost clusters along with the NetVLAD clusters are added, and the vectors belonging to ghost clusters are ignored during the feature aggregation stage. The GhostVLAD-based pooling strategy is used to improve the accuracy of language identification task for Indian languages.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Yes, their model outperforms the state-of-the-art results, achieving an accuracy of 71.8% when applied to the IEMOCAP dataset.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Context tweets are proposed as an additional feature for the feature engineering-based models, while bidirectional GRU networks with LTC modules show promising results in detecting abusive language. Context tweets are extracted from the tweet text and concatenated with the outputs of the labeled tweets for CNN and RNN models. The LTC module is used to extract the context information, which improves the accuracy of classification.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at Facebook pages that had a balance of domains and stances, as well as a variety of pages with different distributions of emotions.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Yes, the task is described as concept-map-based MDS, where a concept map is created from a set of related documents, and the task is to generate a summary in a concept map format. The evaluation is based on the concept map itself, with a focus on evaluating the importance of concepts and their connections in the concept map, as well as the overall quality of the generated summaries. The evaluation is based on a combination of task-specific metrics, such as the concept map size, relevance of the extracted concepts, and the quality of the generated summaries. The evaluation is also based on the concept map itself, with a focus on the usefulness", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "Yes, the proposed approach, GM$\\_$KL, outperforms the previous approaches, including w2g and w2gm, in terms of performance on the benchmark word similarity datasets and entailment datasets. Specifically, it achieves better correlation compared to w2g and w2gm, and better Spearman correlation compared to w2g and w2gm on the entailment datasets. Additionally, it demonstrates better performance on the crowdsourced dataset BIBREF30, while w2g and w2gm perform poorly.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by averaging the predictions from the constituent single models. These single models are selected based on the validation performance, and the resulting model is called a greedy ensemble. The ensemble is then trained on the BookTest dataset, and the final model is the one with the best validation performance. The ensemble is built by taking the average of the predictions of the constituent models, and the model with the highest average performance is chosen as the final model. The ensemble is trained on the BookTest dataset, and the model's performance is evaluated on the standard Children's Book Test dataset. The ensemble can be improved by adding more training data, as", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "SocialNLP 2019 EmotionX and EmotionPush.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset.", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Yes, the proposed system achieves better performance than the strong baseline system.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Dataset: Cognia Interaction norms, which are used in the context of the chatbot, as well as the use of the chatbot, and the use of the chatbot, which is able to respond to the user utterances, and the chatbot, which is able to respond to the user utterances, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to respond to the chatbot, which is able to", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of the SMT and NMT models on the proposed ancient-modern Chinese dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "Three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distributions. These terms are applied to reduce the sensitivity of prior knowledge and make the model more robust.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, CNN with word embeddings, and Recurrent Neural Networks with topic models.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Yes, the neural network-based models with the help of the distributed features (nbow+ and nbow+) improved the performance by several points, resulting in the best performance for the neural network-based models.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Yes, our adaptively sparse attention mechanisms, specifically the $\\alpha$-entmax, lead to a higher degree of interpretability compared to softmax transformers, as demonstrated by the increased average attention weight density vectors and the observation of different types of heads with varying degrees of sparsity, such as sparse and dense heads, and the ability to distribute attention weights in a continuous manner. This property enables the model to better capture the underlying linguistic phenomena captured by the heads, resulting in improved interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the monolingual data used for training the DocRepair model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The evaluation metrics used in this study are:\n\n  * Cross-lingual Natural Language Inference (XNLI) test accuracy\n  * Unsupervised Semantic Attachments Scores (LAS)\n  * Supervised Dependency Parsing (LAS) scores\n\nThese metrics are used to assess the effectiveness of the proposed approach, including the ability to adapt from a pre-trained English model to other languages, effectively fine-tune the model for zero-shot transfer, and evaluate the performance of the model in various downstream NLP tasks.", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "No. The attention module is not pre-trained on the pre-training stage. Instead, it is used in fine-tuning stage.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Cognitive features derived from human eye-tracking measurements, including two primary features and two additional features, are used to enhance sarcasm detection in text. These features include:\n\n1. Simple Gaze (simple features): These features are calculated by recording the fixation duration of the reader on the text, which is normalized to total word count and are used to measure the amount of time spent on the text. The simple gaze features are computed for both sarcastic and non-sarcastic texts, providing a baseline for sarcasm detection.\n\n2. Complex Gaze (complex features): These features are derived from the eye-tracking data and incorporate both linguistic and", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "An LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is a useful source of taxonomic knowledge for taxonomic hierarchy, allowing for definitions and definitions to be constructed from WordNet, which can help understand the structure of the knowledge contained in the underlying language models, enabling a more comprehensive and consistent understanding of concepts and their relationships.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Jasper models were compared to other baselines, including wav2letter, ResNet, and traditional N-gram NLP models, in order to determine the best configuration for Jasper. The best configuration was determined by experimenting with different combinations of normalization techniques, residual connections, and optimization algorithms. The best setting was found to be batch norm with ReLU, followed by residual connections, and then a combination of batch norm and ReLU. This configuration resulted in the best performance on LibriSpeech and competitive results on other benchmarks.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1, UMA, and MRR.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for the ground truth output, including the type of symptom and attribute, as well as the presence or absence of a specific word in the question.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Yes. Expert annotations are required for the task-specific encoder, as it is trained on the task-specific data.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "four machine translation tasks.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "Yes, the ELMo embeddings improve the performance of Estonian in the NER task. The results are presented in Table TABLEREF21.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have experience in the humanities and social sciences, specifically in the areas of computational text analysis, and the challenges and opportunities of working with data from different sources, including the use of “dual use” methods, and the need for careful consideration of the potential for harm, such as the potential for “dual use” of data, and the importance of considering the potential “dual use” of data, and the need to consider the potential “dual use” of data, and the potential for “dual use” of data, and the need to consider the potential “dual use” of data, and the need", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and are similar to the Afrikaans and English, while the Sotho languages are similar to the Xitsonga and Tshivenda.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the performance of the following LSTM models: Layer-wise training, Block-wise model-updating filter, and Blockwise model-updated filter.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Their data set consists of 29,794 articles from Wikipedia, 21,230 arXiv articles, and 2,048 arXiv articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgement assemblies were gathered by evaluating the translation outputs of the RNNSearch and RNNMorph models for various translations in the test set. The results were analyzed and compared to determine the improvements in translation quality, resulting in a 41% increase in the 4-gram, 3-gram, and 2-gram BLEU scores for the RNNSearch model, indicating a significant improvement in translation quality. The human evaluation also showed a 51% improvement in the 4-gram BLEU scores for the RNNSearch model, demonstrating the effectiveness of the model in improving translation quality. The human evaluation", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by measuring the efficiency of a communication scheme, which is the number of keywords retained in the keywords-sentence pair, and the accuracy of the scheme, which is the fraction of sentences generated by the system that are semantically equivalent to the target sentence. The effectiveness of the constrained objective is quantified by comparing the achievable accuracy and efficiency tradeoffs for the objective, which is the sum of the costs and losses. The constrained objective is computed by minimizing the expected cost under a reconstruction loss constraint, and the unconstrained objective is computed by maximizing the expected cost subject to a reconstruction loss constraint. The optimization is performed using a single sample", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are evaluated for the multi-class multi-label classification task.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "source domain: Amazon benchmark, target domain: Amazon benchmark.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art methods, including RAN, QRNN, and NAS, on the PTB and WT-2 datasets.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides three main types of neural network modules: Embedding Layer, Neural Network Layers, and Regularized Layers. The Embedding Layer supports both word/character embedding and additional handcrafted features for enhanced performance. The Neural Network Layers provide a variety of blocks, including RNN, CNN, and Transformer blocks, along with attention mechanisms. The regularized layers, such as Dropout, Layer Normalization, and Batch Normalization, are also included for improving the generalization ability of the models. Additionally, the Block Zoo provides a gallery of reusable blocks for constructing complex network architectures, allowing for customization and reuse of components.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the Carnegie Mellon Pronouncing Dictionary BIBREF13 and the multilingual corpus of orthographic and phonemic data from deri2016grapheme.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used in the experiments for Speculation Cue Detection and Scope Resolution were the baseline models BERT, XLNet, and RoBERTa, which were trained on the same datasets as the experimental setup.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "They use English, Spanish, and French in their experiment.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "tweet2vec outperforms the word-based approach, demonstrating its applicability to various NLP tasks such as named entity recognition (NER), POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes. PolyReponse was evaluated against a set of benchmarked conversational search and response retrieval tasks using Reddit data, including a general domain (Reddit), a structured dataset (Yelp), and a large-scale dataset (OpenSubtitles). The evaluation results showed that PolyReponse outperforms existing methods in terms of both accuracy and efficiency in retrieving relevant responses.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the method of generating maps that reflect the geographic lexical variation across the different states, which reflects the core values of the participants.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "ML methods aim to identify argument components on the argumentation in the context of the gold-standard argumentation theory, such as the Toulmin's model, which is a well-established discourse theory.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Yes.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "unanswerable", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes, the 12 languages.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Two datasets are used to evaluate the effectiveness of the proposed model for forecasting derailment of online conversations, which is a task that involves predicting the future derailment of a conversation based on early warning signs. These datasets consist of two datasets: one is an expanded version of the annotated Wikipedia conversations dataset, which contains a series of controlled conversations with clear-cut classifications of whether the conversation eventually leads to personal attacks or remains civil, and the other is a dataset constructed from the Reddit forum ChangeView, where the model aims to predict whether a conversation will eventually receive moderator intervention due to \"rude or hostile\" behavior.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes. The article mentions that the Portuguese NLP modules were trained using a Portuguese model, which is a deep learning model. Additionally, the SRL module was trained on a Portuguese model, which is also a deep learning model. Therefore, the pipeline components mentioned in the article were based on deep learning models.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated through several quality checks, including sentence-level BLEU scores, word error rate (WER) and character error rate (CER) using the VizSeq BIBREF17, and the ratio of English characters in the translations. Additionally, the dataset is checked for overlapping samples, and the evaluation set is constructed from the Tatoeba corpus. The evaluation set is further filtered by sentence length and number of speakers and accents, making it closer to real-world scenarios and more challenging.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They propose a novel multimodal approach that combines the information from audio and text data in the audio and text modalities separately, utilizing a dual recurrent encoder (ARE) and a multilingual text-to-text encoder (TRE) to simultaneously predict the emotion class of a given signal. The ARE model encodes audio and text data independently, while the TRE model uses the same architecture but focuses on the audio signal. The proposed MDRE model combines the advantages of both models, utilizing the audio information from the ARE and incorporating the text information from the TRE. The model uses a novel attention mechanism to selectively attend to the parts of the transcript that contain", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets going viral refers to tweets that went viral within a certain time frame, specifically, the day of the election of Donald Trump as the 45th president of the United States of America.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "unanswerable", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "the DeepMine project BIBREF4.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Two deep learning models, specifically a 600d ReLU-based DL model and a 1,139d LSTM model, are used in the context of the medical domain for the RQE task.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Yes, the benchmark dataset is the Social Honeypot dataset, and its quality is high.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "LSTM", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is BERT, which achieves F1 scores of 0.673 on both SLC and FLC tasks.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline was a weak baseline without any multilingual data, which consisted of a single NMT model trained on the in-domain parallel data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.4325", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding methods such as word2vec BIBREF9 are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use pre-ordering to match words before reordering them.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Yes. The experts used for annotation were legal experts with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "CNN-RNN-based image-to-poem model and seq2seq models with a dictionary of pre-trained word embeddings for text style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "Yes.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the topics of personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation called \"Extended Middle Context\" that combines the left context, the entity in the relation argument, and the middle context. This context is split into two parts: a combination of the left context, the entity, and the middle context, and a combination of the middle context, the right entity, and the right context. Both contexts are processed by two separate convolutional and maxpooling layers, and the results are concatenated to form the sentence representation.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Yes", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Yes, the resulting annotated data is higher quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "imbalance in analyzed corpora is 33.16%, indicating a disparity in gender representation and speech quantity in French media.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Multi30K BIBREF21", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Yes, the article compares the performance of different baselines in terms of CWS in the closed test setting. The models are BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, and BIBREF10, which are considered as strong baselines for CWS modeling. These models are typically designed with a specific architecture and feature representation, making them suitable for CWS tasks. However, the article does not specify which specific model is compared to others, so the answer", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multi-layer Perceptron.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLTK, Stanford CoreNLP, TwitterNLP, and BIBREF17, BIBREF18, and BIBREF19 for entity-level sentiment analysis.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are conducted on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "existing approaches for modeling geographic locations include bag-of-words representations, term-based representations, and vector space embeddings. The goal of using these methods is to represent locations in a natural way, integrating the textual information from Flickr tags and the available structured information from the dataset. The bag-of-words representation is a simple way to encode locations, while term-based representations capture the relevance of tags to the locations. Vector space embeddings, on the other hand, are used to learn representations of locations in a more general manner, allowing for more flexible and effective representation of locations. In this study, the proposed model combines both bag-of-words representations and vector", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used 3 datasets for evaluation: CSAT, 20 newsgroups, and Fisher Phase 1 corpus.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, BIBREF1, BIBREF2, and BIBREF3 were evaluated in previous work, specifically in the context of subject-verb agreement, \"colorless green ideas\" settings, and manually constructed stimuli, respectively.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "In our approach, the invertibility condition is to ensure that the Jacobian determinant of the Jacobian matrix of the non-linear function is either one or equals to zero, indicating that the projection is invertible. This ensures that the information is preserved and prevents information loss due to the inversion.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Proposed qualitative annotation schema includes features such as redundancy, lexical entailment, factual correctness, complexity, semantic complexity, required reasoning, linguistic complexity, required knowledge, lexical overlap, and other relevant features. The annotation schema is organized into categories such as linguistic features, required reasoning, and factual correctness, and it provides a common framework for evaluating MRC gold standards. The annotation schema is based on the annotation of 50 MRC datasets, with 6 gold standard datasets representing different aspects of the task, such as the answer selection style, required reasoning, and factual correctness. The schema categorizes the data based on the features that are marked", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "Both datasets, WikiSmall and WikiLarge, consist of 82,000 sentences with 11.6 million words each.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Vanilla ST baseline, Pre-training baselines, Encoder-decoder pre-training, Decoder-decoder pre-training, Encoder-pretrained, and Pre-training.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs and neural networks are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 was used for word embedding.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Yes, personalized models outperform baselines in BPE perplexity and UMA, achieving better step ordering and local coherence. Personalization also leads to higher UMA and MRR, with the addition of attention mechanisms improving the semantic plausibility of generated recipes. Personalization is reflected in the generated recipes' coherence scores, which range from 1.78-1.82, and the ability to generate recipes that follow a logical and coherent step ordering. Additionally, the generated recipes are more coherent and acceptable compared to baseline, with a mean recipe-level coherence score of 1.82-1.88.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning includes a reward for content preservation, which is defined as the difference between the sentiment polarity of the input sentence and the sentiment of the generated sentence, and a reward for irony accuracy, which is the absolute value of the difference between the sentiment score of the input sentence and the sentiment score of the generated sentence.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the model may not work well with Shakespearean style transfer as shown in Figure 12, due to the lack of a large collection of Shakespearean prose which describes paintings. They suggest expanding the style transfer dataset to improve the performance of the model.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "Yes, the authors compare their models to three existing benchmarks: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Exposure, characterization, and polarization.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from two datasets: (a) the STAN INLINEFORM0 dataset, which consists of 1,108 unique hashtags from the Stanford Sentiment Analysis 2017 dataset, and (b) the STAN INLINEFORM1 dataset, which contains all 12,594 unique English hashtags and their associated tweets from the Stanford Sentiment Analysis 2017 dataset.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace represents a low-dimensional linear subspace in a word vector space, which retains the majority of the variability of word embeddings, allowing for the separation of words based on their context and preserving the semantic relationships between words. Word subspace can be used to model the word vectors of each class in a text classification task, but it requires a significant amount of data and computational resources to handle the dense and sparse representations of word vectors. The word subspace formulation is a practical and scalable way to represent the content of texts, but it neglects the word semantics, which can be captured by the term-frequency and term-frequency inverse document-frequency weights", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF) BIBREF17.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The article mentions that the post2013 improved introduced a 38-hour Spanish-English ST corpus by augmenting the transcripts of the Fisher and Callhome corpora with English translations. However, the exact number of hours of the Augmented LibriSpeech dataset is not mentioned in the article. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "Fine-grained sentiment classification was part of the SemEval-2016 \"Sentiment Analysis in Twitter\" task, and the dataset for fine-grained classification is split into three parts: train, development, and development_test. The fine-grained sentiment classification is considered harder due to the imbalance in the data and the difficulty of obtaining distant supervision, making it challenging to obtain balanced training data. Therefore, the authors used the high-quality datasets provided by the challenge organizers, including the fine-grained and ternary datasets, and used the data for fine-grained sentiment classification.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Yes.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets from WordNet, WordNetQA, and DictionaryQA, are not subject to quality control, as they are not systematically constructed from a large and diverse set of expert knowledge, the choice of base models, and the use of synthetic data, the choice of question templates, and distractor choices, may not capture the full range of knowledge and reasoning required for a model to be competent in the science domain, as demonstrated by the results in the paper. Additionally, the choice of base models and datasets, as well as the choice of question templates and distractor choices, can affect the quality of the generated datasets,", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive, with the best model achieving a micro-average f-score of 0.368.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed in this work is a combination of two tags: INLINEFORM0 and INLINEFORM1. The tag sequence consists of the tag INLINEFORM0 indicating that the current word is not a pun and the tag INLINEFORM1 indicating that the current word is a pun. If the tag sequence contains a tag sequence that contains a tag INLINEFORM0, the text contains a pun and the word corresponding to INLINEFORM1 is the pun.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness refers to the ability of a model to reduce the sensitivity to prior knowledge, which is typically represented by the number of labeled features and the KL divergence between the reference distribution and the model's predicted distribution. By incorporating the labeled features and KL divergence regularization terms, the model can be made more robust and capable of handling unbalanced labeled features and class distributions, leading to better performance. Regularizing the KL divergence further allows for the incorporation of additional knowledge, making the model more interpretable and less biased towards the desired class distribution. Overall, the proposed approach demonstrates the effectiveness of using prior knowledge to enhance the robustness of a model,", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "Other sentence embeddings methods are evaluated, including the siamese and triplet networks, which are used in the argumentative setting. These methods are compared to cosine-similarity and the maximum output of the sentence embeddings. The siamese network architecture is used to derive fixed-sized sentence embeddings, while the triplet network is used to find the most similar sentence pairs in a collection of 10,000 sentences. The authors evaluate the performance of these methods on the STS benchmark and the AFS dataset, achieving an improvement of 11.7 points on the STS benchmark and 5.5 on the AFS dataset", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Yes, the proposed DSC loss outperforms the best baseline results by a large margin on CTB5, CTB6, and CTB4, indicating that the proposed DSC loss is robust to the data imbalance issue in NER tasks. The proposed DSC loss outperforms BERT-tagger by +1.8 on CTB5, +1.8 on CTB6, and +2.19 on CD on CTB4, +0.29 on CD5, and +0.96 on CD6. The proposed DSC loss achieves SOTA performance on all of the three", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: ranking questions in Bing's People Also Ask and classification questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Yes, they compared their model with other baseline models, including RvNNs, trees-based models, and Gumbel Tree-LSTM, as well as the recently proposed latent tree-structured models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "Core relation (chain) for each topic entity selection.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Name-based nearest-neighbor model (NN)", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "Unwarranted inferences are identified in the Flickr30K dataset, including stereotypes-driven descriptions, which involve excessive use of adjectives to describe individuals who do not conform to traditional gender roles or exhibit unusual behaviors. Other methods include analyzing the use of adjectives and identifying patterns related to old people, ethnicity, and age groups. Detecting bias in language requires manual inspection of descriptions and tagging of phrases with part-of-speech information. A coreference graph can be created to identify referring expressions, and a coreference-based approach can help identify the richness of the data and the potential for bias.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "unanswerable. The article discusses the Winograd Schema Challenge, which is a competition for machine translation programs to identify the correct referent for an ambiguous pronoun in a sentence. The article does not mention any specific language in which the challenge is explored.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n- Models that use not only hidden states but also cell states from the previous layer: Baseline (plain stacked LSTM)\n- Models with different INLINEFORM0: (i) models that do not include INLINEFORM0, and (ii) models that incorporate INLINEFORM0\n- Models that integrate lower contexts via peephole connections: (iii) models that remove INLINEFORM1 and use only INLINEFORM2, and (iv) models that combine lower contexts via peephole connections\nNote that the proposed architecture (CAS-LSTM) is not included in the list of models tested.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with ILP-based summarization algorithms, including Sumy, Sumy, and Sumy, which require a number of sentences kept in the final summary and a suitable value of INLINEFORM0.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "unanswerable", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "unanswerable.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Yes, they experiment with 7 Indian languages.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Yes.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "Yes, the proposed Human Level Attributes (HLA) model outperforms the baselines, achieving a performance difference of around 40.67% in Hits@n/N, demonstrating the effectiveness of the proposed HLA model in generating the language style of specific characters. Additionally, the HLA-OG guidance helps in improving the performance of the CSM component, leading to a higher accuracy of 25.08% in the CSM evaluation. The HLA-OLRAM model outperforms the baselines, achieving a mean score of 40.67% in the HLA-OG evaluation", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Yes. Our model gains significant improvements over the baselines, especially in terms of self-bleu, through the addition of reward augmentation and the use of a stationary distribution in the reward function. The stability of the training process is ensured by the constraint of the reward function, which limits the search space and prevents the generator from being trapped in the vicinity of the real data, making the generator more stable and capable of generating diverse text samples.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can detect biases in the process of collecting or annotating datasets, specifically in the case of Waseem and Hovey's dataset, where the model mislabels tweets as racist, sexist, nor neither, and vice versa, indicating that the model does not capture the subtle nuances of hate speech and its underlying context. Additionally, the authors mention that the model struggles to identify implicit abuses in the dataset, leading to biased results. This suggests that the model may not be able to capture the full range of biases in the data, making it necessary to investigate the impact of different fine-tuning strategies on the", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "OurNepali dataset is in standard CoNLL-2003 IO format, consisting of 14 million words from books, web-texts, and news papers, and 16225 unique words extracted from the POS-tagged Nepali National Corpus.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Yes, the proposed method improves the F1 score for paraphrase identification by introducing a hard version of F1 score, which effectively addresses the dominating effect of easy-negative examples on the task. The proposed DSC loss outperforms other losses like dice loss and focal loss, demonstrating its ability to tackle the data imbalance issue in NLP tasks.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Multitask learning with eye-tracking, self-paced reading time, and ERP data is used.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " Stimulus-based EEG data was presented to the subjects to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Yes, Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+RL, and Pointer-Gen+RL-ROUGE are used for evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Traditional machine learning models, including Word-Level, Character-Level, and Hybrid CNN, are used for feature engineering. Additionally, RNN-based models, such as Bidirectional GRU, are investigated for their ability to handle context tweets. Ensemble models, such as GBT and RF, are also explored for improved performance.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "bi-directional language models", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The proposed method dynamically adjusts weights of training examples according to their corresponding probabilities, with a weight proportional to the difference between the probabilities of positive and negative examples. This approach addresses the dominating effect of easy-negative examples and ensures that the model pays more attention to hard-negative examples, resulting in improved performance on both easy-negative examples and hard-negative examples. The method employs a weight adjustment strategy that assigns different classes to different weights, with the goal of minimizing the dominance of easy-negative examples and promoting the learning of hard-negative examples. The weights are determined by the inverse class frequency or the weight of the Tversky index, which balances precision and recall and", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Yes, the proposed strategies, including the ones based on the knowledge graph, significantly outperform the baseline methods in solving the bottlenecks in the state-action space of Zork1. Specifically, the knowledge graph-based methods achieve a score of around 40, whereas the other methods get stuck in bottlenecks and do not surpass them. The improved performance of these methods is attributed to the ability of the knowledge graph to guide the agent towards the next optimal state, which is more likely to lead to a higher reward. Additionally, the knowledge graph-based methods leverage the knowledge graph to enhance the exploration process, allowing the agent to traverse through", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a monolingual model and a multilingual model. The monolingual model uses a Bayesian model for the primary role ordering and repetition preferences, while the multilingual model uses word alignments between sentences in parallel corpora to capture cross-lingual role correspondences.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard orthographic transcriptions are identified by analyzing the presence of disambiguated words, such as foreign words, in the text. These words are marked with labels that indicate the intended meaning, but may contain non-standard orthographic variations, such as misspellings, which are considered to be valuable for understanding the context and improving the accuracy of speech recognition and machine translation systems.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar-based RNN (ScRNN) is a type of recurrent neural network (RNN) for word recognition, specifically designed to handle misspelled words. The model processes a sentence of words with misspelled characters, treating the first and last characters independently, and the internal characters as a bag of characters. The input representation is obtained by concatenating the internal characters with the external characters, and the word recognition model is trained on a smaller corpus. The word recognition model is trained on a specific domain, and the word error rate (WER) is measured on a test set. The word error rate is a measure", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages are explored.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "Yes, NCEL achieves significant improvements in efficiency and generalization compared to other existing approaches, achieving promising performance in various datasets.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the baseline used in the article is the same as the one reported by Felice2014a, which is the approach used to train error detection systems.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the clinical notes from the 2010 i2b2/VA dataset.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the decoder to generate sequences with a complete context, which helps alleviate the problem of incomplete and inconsistent context in the previous abstractive methods. By masking words in less attended sentences, the decoder can generate more consistent and complete context-aware word representations, which can lead to better generation of summary contexts and thus improve the quality of generated summaries. Additionally, by feeding the summary draft with both context representations, the model can leverage the pre-trained contextual embeddings in BERT, which can help the decoder generate more coherent and fluent sequences, reducing the exposure bias problem. Therefore, masking words in the decoder", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The article mentions that the SDAE model uses a Twitter dataset, specifically the Twitter textual entailment task. Therefore, the answer is \"yes\".", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF and LDA are used to extract and encode the text content of pathology reports into structured data. TF-IDF is a term frequency-inverse document frequency measure that assigns higher weights to terms that appear frequently in a document while ignoring terms that appear infrequently. LDA is a Latent Dirichlet Allocation algorithm that groups similar words into topics based on their TF-IDF weights, enabling identification of important keywords within the reports.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 tweets representing 9,300 tweets. Each tweet is annotated with one or more depressive symptoms, for example, depressed mood, disturbed sleep, or fatigue or loss of energy.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "Yes.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated to Spanish.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "Content-based classifier in conjunction with two feature selection techniques.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Baseline for the FLC task is a simple logistic regression model with default parameters, where a word can belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. This baseline system demonstrates the inadequacy of a straightforward approach to the task.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with the baseline of a rule-based system that scores candidate words according to eleven simple heuristics, including the phonetic distance via the CMU Pronouncing Dictionary, the state-of-the-art system for homographic pun location, and the state-of-the-art system for pun detection and location.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Yes. The methodology includes a set of features which reflect the presence of different sources, including political bias, in the classification task.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient-modern Chinese dataset comes from the internet, specifically from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "UTCNN has three layers: a convolutional layer, a max pooling layer, and a fully connected layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF7 , which focuses on predicting climate-related features, such as temperature and land cover types, from a set of locations.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a Spanish clinical narrative dataset that contains real-world medical documents, while MEDDOCAN is a shared task dataset that focuses on the detection and classification of sensitive information in Spanish clinical narratives.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used simple and complex gaze-based features, including unigrams and pragmatic features, as well as two additional linguistic features: readability and word count, to enhance sarcasm detection in text.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "OKBC is the basis for open-world knowledge learning and inference in conversations, making it a key component of the generic and effective chatbot learning mechanism.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe are the targets of the tweets in the stance-annotated tweet dataset.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Additional experiments are conducted, including error analysis and investigation of improvements in the model's performance. These experiments include conducting automatic evaluations and human evaluations, analyzing the quality of generated sentences, and investigating the effectiveness of different models in transforming irony sentences to non-ironic sentences while preserving content and sentiment polarity. Additionally, the study investigates the difficulty of ironies in different categories and proposes a specific approach to transfer irony from ironic sentences to non-ironic sentences in an unsupervised manner. The proposed method combines style transfer and reinforcement learning to control the content and sentiment polarity of the input sentence, ensuring that the model generates ironic sentences while preserving the content", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of the attention mechanism in Transformer BIBREF24 which replaces the standard self-attention layer with a bi-affine attention scorer to capture the local and directional information. The bi-affine attention scorer assigns a probability to each position of characters, which is influenced by the local and directional information. The bi-affine attention is achieved by adding a bias term to the self-attention layer, which ensures that the relationship between two characters with distant characters is weaker than adjacent characters. The bi-affine attention is applied in a greedy decoding manner to find the boundaries of words, ensuring the segmentation", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "social media.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the base models used in the proposed framework, including the pre-trained CNN-SVM model and the difference-based CNN model.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "No.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were always the same, ranging between 0.716 and 0.718 for the five subtasks.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which are approximately 19,550 tokens in total.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "text classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Their model is compared to handcrafted rules-based question classification methods.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "These versions of ELMo have a significantly larger training set than the previous ones, with 20 million tokens in total, compared to 20 million tokens in the previous ELMo models.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "64, 16, 20, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, ", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, and MWMOTE.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.4325", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "Yes. The article states that there are high-level abstractions in the popular deep learning frameworks, such as TensorFlow, PyTorch, and Keras, which make it difficult for engineers to master these frameworks and focus on building custom models. This leads to a lack of flexibility in the development process, making it challenging to create efficient and customizable models for NLP tasks. The article suggests that the use of higher-level abstractions and standard components in the Block Zoo can help alleviate these challenges and enable engineers to build complex models with ease.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "Yes, our proposed KBQA system achieves the state of the art on both SimpleQuestions and WebQSP benchmarks. Specifically, our system outperforms the previous state-of-the-art methods on both benchmarks, achieving 91.2% and 88.9% on SimpleQuestions and 80.65% on WebQSP, respectively. Additionally, our system outperforms the state-of-the-art on WebQSP by 2-3% and 2, showing that our KB relation detection model is able to capture the global information and handle long-range relations. Furthermore, our system outperforms the state", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
