{"pred": "unanswerable.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which was originally proposed for face recognition, to improve language identification performance for Indian languages. It is an extension of the NetVLAD approach, which consists of Ghost clusters along with NetVLAD clusters. Ghost clusters are added along with NetVLAD clusters, and the resulting representation has improved language discrimination capacity.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Yes, their model outperforms the state of the art results.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Context tweets are proposed as an additional feature for detecting abusive language in the article. They are extracted from the tweet text and concatenated with the outputs of their corresponding labeled tweets. These context tweets are considered during the training process, where they are treated as additional information to enhance the classification accuracy.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at Facebook pages that had a balance of news, cooking, and entertainment pages, as well as pages with a focus on news and opinion.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "Yes, the task is described as concept-map-based MDS, where a concept map is created for each document cluster in the form of a concept map, and the task is evaluated using a combination of concept map quality and user response scores. The evaluation is based on a concept-map-based MDS dataset created from 30 topics, each with 40 documents and a concept map that connects the concepts and their relations. The dataset is split into two tasks: concept-map-based MDS and concept-map creation, and the concept-map-based MDS is evaluated using a reference summary evaluation and a concept-map creation task. The dataset is publicly", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "Yes, the proposed approach, GM$\\_$KL, outperforms the previous approaches, including w2g and w2gm, in terms of performance on the benchmark word similarity datasets and entailment datasets. The proposed approach uses a KL approximation between Gaussian mixtures, which provides a stricter upper and lower bound on KL between Gaussian mixtures compared to the original KL approximation in w2g and w2gm approaches. Additionally, the proposed approach outperforms the w2g approach in terms of performance on the benchmark word similarity datasets and the entailment datasets, while w2g and w2gm approaches achieve better correlation scores than the", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by averaging the predictions from the constituent single models. The model is first trained on the original CBT training data, and then the best performing model is added to the ensemble if it improves the validation performance. The ensemble is then updated with the best model until convergence, and the final model is chosen based on the average performance on the CBT validation and test datasets.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "SocialNLP 2019 EmotionX consists of two subsets, Friends and EmotionPush, according to the source of the dialogues.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "Yes, the proposed system outperforms strong baseline systems.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Dataset is used.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "Yes", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of the SMT and NMT models on the proposed dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "Three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distributions. These terms are used to reduce the sensitivity of the prior knowledge and make the model more robust. Regularizing the neutral features with the uniform distribution ensures that the model does not bias towards the majority of labeled features, while the maximum entropy and KL divergence terms aim to control the unbalance in the labeled features and the predicted class distribution, respectively.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, CNN, Recurrent Convolutional Neural Networks, and the above-mentioned baselines.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "Yes, the neural network-based models improved the performance by a significant margin, achieving the best performance with the use of the multitask learning approach.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Yes, their model improves interpretability by allowing the shape and sparsity of attention heads to be learned automatically, resulting in more diverse and concentrated attention patterns, especially in the encoder self-attention and decoder self-attention modules.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the model trained on monolingual data.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article mentions that the LAzed Attachment Scores (LAS) are used for evaluating the performance of RAMEN models. The article also provides table tabREF32 showing the results of RAMEN models on XNLI and dependency parsing tasks, where the LAS scores are compared to mBERT and the baseline model. The article states that the LAS scores are computed as the Labeled Attachment Scores (LAS) for zero-shot transfer. The article also discusses the importance of initializing the embeddings from the English word-embedding space, which affects the performance of the model. The article suggests that the best initialization is achieved by RAMEN$_{\\textsc {", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "No.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Cognitive features derived from human eye-tracking measurements, including two primary features and two additional linguistic features, are used to enhance sarcasm detection in text in contrast to literal texts.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "An LSTM.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is a useful source of taxonomic knowledge for taxonomic hierarchy, allowing for definitions and definitions to be identified and analyzed, as well as for understanding the relationship between concepts and their definitions. However, the effectiveness of WordNetQA datasets in terms of definitions and taxonomic hierarchies may not be sufficient for assessing the overall competence of transformers-based models, especially those trained on large-scale datasets, and the need for a diverse set of systematic datasets to evaluate model competence.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Jasper was built on top of prior work that includes wav2letter, which uses a stack of 1D-convolution and ReLU, batch normalization, and dropout. The authors compared different normalization methods, including batch norm, weight norm, and layer norm, and found batch norm to be the most effective for training.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1, UMA, and MRR.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create labels for the ground truth output, including the position indices of tokens, to indicate the position of the symptom and attribute in the question.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "Yes, the task-specific sentence encoder needs to be learned from scratch for the difficulty prediction task.", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "four machine translation tasks.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "Yes, the ELMo embeddings improve the performance of Estonian in the NER task.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have experience in the humanities and social sciences, specifically in the areas of computational text analysis, and the challenges and opportunities of working with these data.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and are similar to the Afrikaans and English.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the performance of deep LSTM RNNs with LSTM RNNs, as well as LSTM RNNs with Xavier initialization and block-wise pre-training.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Their data set consists of 29,794 articles from Wikipedia, with a dataset of 29,794 articles.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human evaluations were performed using a variety of metrics, including BLEU scores, fluency, and relative ranking values, to assess the quality of the translation outputs. The results showed that the RNNMorph model, specifically the RNNMorph, produced better results than the phrase-based SMT system, indicating the importance of attention mechanisms in improving the translation quality. The human evaluation results showed that the human judgments were consistent across the translations, with the RNNMorph producing better results than the phrase-based SMT system. The human judgment was based on the quality of the translation output, including the reduction in the number of words,", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by measuring the efficiency of a communication scheme, which is the number of keywords retained in the keywords-sentence pair, and the accuracy of the scheme, which is the fraction of sentences generated by the system that are semantically equivalent to the target sentence. The effectiveness of the constrained objective is quantified by comparing the achievable accuracy and efficiency tradeoffs for the objective, which is the sum of the costs and losses. The authors compare the constrained objective to two baselines, Unif and Stopword, and observe that the constrained objective outperforms the Unif and Stopword baselines, demonstrating the benefits of jointly learning the encoder", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, recall, and F-measure are evaluated for the multi-class multi-label classification tasks.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "source domain: Amazon benchmark, target domain: Amazon benchmark.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare with state-of-the-art models such as RAN, QRNN, and NAS, demonstrating that PRUs can achieve similar or better performance than these models.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides a two-layer solution to cover three typical engineer personas, covering various NLP tasks, including sentence classification, text classification, sequence labeling, knowledge distillation, and extractive machine comprehension. The toolkit offers a range of end-to-end network templates, including RNN, CNN, and Transformer blocks, along with additional support for attention mechanisms, loss functions, and model optimization. The Block Zoo provides a gallery of standard and reusable blocks, allowing for easy customization and reuse of complex network architectures.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the Carnegie Mellon Pronouncing Dictionary BIBREF13 and the multilingual corpus of deri2016grapheme for all languages.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines used in the experiments for Speculation Cue Detection and Scope Resolution were the baseline models BERT, XLNet, and RoBERTa, which were trained on the respective datasets.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "They use English, Spanish, and French in their experiment.", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "tweet2vec outperforms the word-based approach, demonstrating its applicability to various NLP tasks such as named entity recognition (NER), POS tagging, text classification, and language modeling.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes. PolyReponse was evaluated against a set of baseline systems, including a Reddit dataset, a Yelp dataset, and a set of intent classifiers. The evaluation results showed that the PolyReponse system outperforms these baselines in terms of text and photo responses, and achieves a higher overall accuracy in the ranking task.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the method of generating maps that reflect the geographic lexical variation across the different states, which can reveal the distribution of psycholinguistic and semantic word classes.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "ML methods aim to identify argument components on the discourse level.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "Yes.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "unanswerable", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "Yes.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Two datasets are applied to model derailment of online conversations. The Wikipedia data and the ChangeView dataset are constructed from the Reddit API and involve the task of forecasting whether a conversation will eventually have a comment removed due to \"rude or hostile\" behavior. The dataset is divided into 6,842 conversations, which are split into 600,000 conversations from the CMV subreddit, and 6,842 conversations from the Wikipedia conversations.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated through several quality checks, including sentence-level BLEU scores, word error rate (WER) and character error rate (CER) using the VizSeq BIBREF16, and the ratio of English characters in the translations. Additionally, the dataset is checked for overlaps between CoVoST and the development/test sets, and the evaluation set is constructed from the Tatoeba corpus. The evaluation set is also checked for consistency in speaker demographics, while the baseline models are tested using the fairseq BIBREF20 and SpecAugment BIBREF21. The BLEU scores", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They propose a novel multimodal approach that combines high-level text and low-level audio signals to create a more comprehensive representation of the input data.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improves by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets going viral refers to tweets that went viral within a certain time frame, specifically the day of the election of Donald Trump as the 45th president of the United States of America.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "unanswerable", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "the DeepMine project, which was developed at the beginning of 2017 and ended in the middle of 2018. The database was built using crowdsourcing and the data collection was conducted using an Android application.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Two deep learning models, specifically a 600d ReLU-based RQE model and a 1,139 question pairs, are used in this study, while the best system achieved 98.6% accuracy on the TREC 2017 medical task.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "Yes, the benchmark dataset is the Social Honeypot dataset, and its quality is high.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "LSTM", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is BERT, which achieves state-of-the-art performance in FLC task. It outperforms other models (CNN and Logistic Regression) in terms of F1 score on dev (internal) and dev (external) datasets.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline was a weak baseline without any multilingual data, which is the result of a strong baseline without any in-domain data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.4325", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding methods such as word2vec BIBREF9 are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use pre-ordering to match the word order of the source language and train the parent model on the pre-ordered sentences.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes.", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Yes.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "CNNs, sentiment CNN, and scene CNN are used for painting embedding, while a dictionary-based approach is used for language style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "Yes.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the topics of personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose a new context representation called \"Extended Middle Context\" that combines the left context, the entity in the relation argument, and the middle context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Yes, the dataset contains six different entity types: Person, Location, Organization, and MISC.", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "Yes, the resulting annotated data is higher quality.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "imbalance in analyzed corpora is 33% of the total speech turns, indicating a disparity in gender representation.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Multi30K", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Yes, the article compares the performance of different baselines models for CWS in terms of closed test setting. The models are BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, and BIBREF10, which are the models used in the traditional and neural models, respectively. The article states that the main difference between the neural and traditional models lies in the way they handle the word representation, with the traditional models relying on diverse $n$-gram features while", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multi-layer Perceptron.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They use NLP toolkits such as NLTK, Stanford NLP, TwitterNLP, and BIBREF23, BIBREF27, BIBREF28, and BIBREF29 for entity-level sentiment analysis.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are performed on the SQuAD dataset BIBREF3.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "existing approaches for modeling geographic locations using Flickr tags, numerical features, and structured datasets include:\n\n  * BIBREF7, where bag-of-words representations were found to be effective for predicting climate-related features, while the term selection method was crucial for capturing the relevant aspects of geographic locations.\n  * BIBREF44, where a combination of word embeddings and geographic locations was proposed, finding that the proposed method captures the same information as the original high-dimensional datasets.\n  * BIBREF43, where a term-based approach was used to learn geographic locations, finding that the proposed method can effectively capture the relevant aspects of the locations", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used 3 datasets for evaluation: CSAT, 20 newsgroups, and Fisher.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset.", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, BIBREF1, BIBREF2, and BIBREF3 were evaluated in previous work, specifically in the case of subject-verb agreement, in a “colorless green ideas” setting in which content words are replaced with random words with the same part-of-speech and inflection, and the focus verb is marked with the absolute position embedding.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "In our approach, the invertibility condition is to ensure that the Jacobian determinant of the Jacobian matrix of the non-linear function is either zero or invertible, and the Jacobian regularization term ensures that the projected data lies in the latent space. This condition allows us to derive exact inference and marginal likelihood computation procedures for the non-unsupervised model, enabling tractable exact inference and marginal likelihood computation.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "Proposed qualitative annotation schema includes features such as redundancy, lexical entailment, and factual correctness, as well as a range of reasoning and knowledge required to answer the question. The schema categorizes the types of reasoning required by a gold standard, marking the type of reasoning required for the task of reading comprehension, and taking into account the required reasoning and knowledge. The annotation schema is based on the characteristics of the gold standard, such as the amount of required reasoning, the presence of lexical cues, and the required knowledge, and provides a common framework for evaluating MRC gold standards. The proposed annotation schema is described in Table TABLEREF19,", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "Both datasets are 82,000 sentences with 11.6 million words, and 2,359 sentences split into 2,000 for training and 359 for testing.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Vanilla ST baseline, pre-training baselines, and pre-training + fine-tuning baselines.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs and neural networks are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 was used for word embedding.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Yes, the personalized models outperform the baselines in the dataset. Personalization improves the coherence and readability of generated recipes, resulting in a higher likelihood of user satisfaction. The human evaluation surveys showed that personalized models generated more coherent and preferred recipes compared to baseline models.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning includes a binary reward to control the irony accuracy and a sentiment reward to control sentiment preservation.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the model may not work well with Shakespearean style transfer as shown in Figure 12, due to the lack of a large collection of Shakespearean prose which describes paintings. They suggest expanding the style transfer dataset to improve the performance of the model.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "Yes, the authors compare their models to three existing benchmarks: the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "Exposure: The distribution of tweets labeled as containing fake news was significantly lower than those labeled as not containing fake news. The proportion of accounts spreading fake news was 12.5%, while the proportion of accounts not containing fake news was 87.5%. The number of friends/followers of the accounts spreading fake news was 2.5 times higher than those not spreading fake news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Task (STAN INLINEFORM0) dataset, which consists of 12,128 hashtags and their associated tweets from the year 2019.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace represents a low-dimensional subspace in a word vector space, which captures the semantic relationships between words and retains the context of the words within a document. Word vectors are learned from a corpus and used to create a set of word vectors for each class, allowing for the distance between words to be calculated and used to measure similarity between word vectors. The word subspace formulation is a practical and compact way to represent text data, but it neglects the word semantics, leading to the need for a word subspace representation that takes into account word frequencies. The word subspace is mathematically defined as a low-dimensional linear subspace", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF) BIBREF16", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The article does not mention the size of the Augmented LibriSpeech dataset. Therefore, the answer is \"unanswerable\".", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "Fine-grained sentiment classification was part of the SemEval-2016 \"Sentiment Analysis in Twitter\" task, and the dataset for fine-grained classification is split into three parts: train, development, and development_test.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Yes.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets from WordNet, WordNetQA, and DictionaryQA, are not subject to quality control, as they are not systematically constructed from a large and diverse set of expert knowledge, the choice of base models, and the use of synthetic data, the choice of WordNetQA datasets, and the choice of WordNetQA datasets, are not controlled, and the data is not systematically generated from a large set of structured knowledge, the choice-based models, and the choice-based models, the choice-based models, and the choice-based models, the choice-based models, are not controlled, and the data is not systematically generated", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their performance on emotion detection was competitive, with the best model achieving micro-average precision, recall, and f-score of 0.368, micro-average.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed in the article is the INLINEFORM2 scheme, which consists of three tags: INLINEFORM0, INLINEFORM1, and INLINEFORM3. The tag sequence for a context from the training set indicates that the text contains exactly one pun and the word corresponding to INLINEFORM1 is the pun.", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness refers to the ability of a model to reduce the sensitivity of prior knowledge and make the model more practical, ensuring that the model is less affected by the presence of biased prior knowledge. Regularization terms such as neutral features, maximum entropy, and KL divergence are applied to the model's expectations, encouraging the model to prefer the model distribution over the reference distribution, which helps the model interpret the unbalanced and unlabelled data. By incorporating these regularization terms, the model can effectively reduce the sensitivity to the prior knowledge and make the model more robust.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "Other sentence embeddings methods are evaluated in the section titled \"Semantic Textual Similarity (STS)\". These methods include:\n\n  * Skip-Thought BIBREF12, which trains a siamese bi-directional long short-term memory (BiLSTM) model to predict the surrounding sentences.\n  * Universal Sentence Encoder BIBREF11, which uses a transformer network and a multi-layered feedforward network to generate fixed-size sentence embeddings.\n  * Infersent BIBREF1, which trains a siamese bi-directional long-term memory (BiLSTM) model to predict the surrounding sentences.\n  * Universal Sentence", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "Yes, the proposed DSC loss outperforms the best baseline results by a large margin on CTB5, CTB6, and CTB4, indicating the three losses are not consistently robust in resolving the data imbalance issue. The proposed DSC loss performs as a hard version of F1 score, which enhances the discrimination between positive and negative examples, and alleviates the dominating effect of easy-negative examples on the SOTA results on the tasks of part-of-speech tagging, named entity recognition, machine reading comprehension, and paraphrase identification.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: ranking questions in Bing's People Also Ask and classification questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "Yes, the baseline models included in the ablation study were sentence-level sentence representations generated by the vanilla RvNN, the tag-level tree-LSTM, and the tag-based model BIBREF10 ( BIBREF10 ) , which applies a tag-based model to generate sentence representations for each node in a tree.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "Core component for KBQA is the Hierarchical Matching Between Question and Relation (HR-BiLSTM) model, which generates the highest-scored relation pairs from the question and relation embeddings, enabling the KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Name-based nearest-neighbor model (NN)", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "Unwarranted inferences are identified in the article, specifically in the form of stereotypes and speculations about the content of an image, which are propagated by language models trained on the Flickr30K dataset.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "unanswerable. The article discusses the Winograd Schema Challenge, which is a competition for machine translation programs to identify the correct referent for an ambiguous pronoun in a target language. The article does not mention any specific language in which the concept of the Winograd Schema is explored.", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models: Sentence Encoder, Multilingual RNNs, and Bidirectional Stacked LSTM.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with ILP-based summarization algorithms provided by Sumy package.", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "unanswerable", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "unanswerable", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "DTA18 and DTA19.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Yes, they experiment with 7 Indian languages.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Yes.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "Yes, the proposed Human Level Attributes (HLAs) model outperforms the baselines, achieving a significant improvement in accuracy at retrieving the ground truth response from the target character, demonstrating that the proposed HLA model is able to accurately capture the language styles of characters based on their traits, whereas the baselines rely on a high-level personality traits from TV Tropes, which lacks the detail and context of character-specific traits.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "Yes. Our model gains significant improvements over the baselines, achieving better performance in terms of forward perplexity and Self-BLEU scores.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can detect biases in the process of collecting or annotating datasets, as some datasets contain slurs and terms related to certain groups, leading to different types of biases in the annotations. This can be seen in the dataset of Waseem and Hovey BIBREF10, where the model can detect biases in the process of collecting and annotating datasets, particularly when analyzing the dataset of Waseem and Hovey BIBREF10, which contains slurs and terms related to religious, sexual, gender, and ethnic minorities, leading to different types of biases in the dataset.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "OurNepali dataset is in standard CoNLL-2003 IO format, which consists of 14 million words from books, web-texts, and news papers.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Yes, the proposed method improves the F1 score for paraphrase identification by using the proposed DSC loss, which outperforms other methods like BERT and XLNet by +0.73 and +0.86, respectively.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "Multitask learning.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": " Stimulus-based EEG data was presented to the subjects to elicit event-related responses.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Yes, Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Pos-FT, Pointer-Gen+Pos, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SENS, Pointer-Gen+ARL-SENS, and Pointer-Gen+ARL-ROUGE are used for the evaluation.", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Traditional machine learning models and neural network models are used on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "bi-directional language models", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The proposed method dynamically adjusts weights of training examples based on the ratio of $p$ to $\\alpha$ or $\\beta$, where $\\alpha$ and $\\beta$ are hyperparameters that can be selected by cross-validation. The weights are dynamically changed according to the trade-off between easy-negative examples and easy-positive examples, which helps to emphasize confident examples and reduce the dominance of easy-negative examples, thereby alleviating the dominating effect of easy-negative examples.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "Yes, the proposed strategies achieve improved performance compared to the baseline A2C and A2C-chained methods. Specifically, the two methods that incorporate the knowledge graph significantly outperform the A2C and A2C-chained methods, demonstrating the importance of leveraging knowledge graphs in more efficient exploration. Additionally, the A2C-Explore approach demonstrates that the knowledge graph plays a crucial role in discovering promising states and helping the agent navigate through bottlenecks, leading to a significantly faster exploration process and a higher reward trajectory.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a single Bayesian model for each language, and the corresponding multilingual and cross-lingual latent variables, capturing cross-lingual semantic role patterns.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard orthographic transcriptions are identified by analyzing the presence of disambiguated words, such as foreign words, in the text, and identifying the corresponding characters in the dictionary. These annotations help in understanding the range of possibilities within the language and can be used for correcting and normalizing orthographic conventions.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar-based RNN (ScRNN) is a word recognition model that processes a sentence with misspelled words by concatenating the internal characters of the words and the first and last characters, and processing the resulting sequence of characters using a bi-directional LSTM. The model is agnostic to the order of the internal characters, allowing it to process sentences with varying internal character sequences.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages are explored.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "Yes, NCEL achieves the best performance across different datasets, achieving an average improvement of 2% on Micro F1 and 3% Macro F1 in the evaluation metrics. The model is also efficient in terms of computational complexity, with a negligible time complexity in the end-to-end process. Additionally, the model is able to effectively leverage global information in the reference graph, enabling the identification of the correct entity as the correct entity for the mention英格兰 cricket team, which is a crucial step towards achieving the desired topic coherence. Overall, the study demonstrates the effectiveness of NCEL in addressing the challenges of entity linking, particularly in the presence of", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "Yes, the baseline used in the article is the same as the one reported by Felice2014a, which is the CoNLL 2014 shared task benchmark.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the clinical notes from the 2010 i2b2/VA dataset.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the decoder to generate each word in the summary one word at a time, which can help alleviate the repetition and incoherence problem caused by repeating phrases in the summary, which can lead to poor quality summaries. By focusing on one word at a time, the decoder can generate more coherent and context-aware sentences, which can improve the quality of generated summaries.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The authors in BIBREF21 use a comprehensive analysis of models spanning the range of complexity from word averaging to LSTMs for transfer learning after optimizing a margin-based loss on PPDB. For transfer learning, they work with a pre-trained model like STV, FastSent, Siamese CBOW, and SDAE, which are trained on a dataset of tweets and work well on sentiment classification, sentiment analysis, and text entailment tasks.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF and LDA are used to extract features from pathology reports. TF-IDF is a term frequency-inverse document frequency measure that highlights the importance of a term in a corpus, while LDA is a Latent Dirichlet Allocation algorithm that groups similar terms into topics.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 tweets representing 9,300 tweets. Each tweet is annotated with one or more depressive symptoms, for example, postpartum depression, major depressive disorder, and major depressive disorder.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "Yes.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated to Spanish.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "Content-Based Classifier with Two Feature Selection Techniques", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "Baseline for the FLC task is a simple logistic regression model with default parameters, where a word can belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. The team qianlp BIBREF40 based their approach on a word-level classification with BERT BIBREF40, a word embedding-based approach.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with the baseline of a rule-based system for pun detection and a weakly supervised approach to pun location.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Yes. The methodology includes a set of features which reflect the political bias of different sources, including the classification of news articles as either disinformation or mainstream, and the categorization of news articles based on the source of the news.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient-modern Chinese dataset comes from 10K aligned bilingual paragraph pairs written in ancient and modern Chinese, which are extracted from ancient texts in several dynasties and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "UTCNN has three layers: a convolutional layer, a max pooling layer, and a fully connected layer.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the 70 million Flickr photos with coordinates in Europe, which is the basis for the model used to learn geographic location vectors.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two datasets: NUBes-PHI and MEDDOCAN. NUBes-PHI is a Spanish clinical narrative dataset that contains real medical records, while MEDDOCAN is a shared task dataset that involves the detection and classification of sensitive information in medical documents.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used simple and complex features derived from the text, including unigrams and pragmatic features, as well as the addition of contextual information from tweet data.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "OKBC is the basis for open-world knowledge learning in human-machine conversations.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes.", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe are the targets of the tweets in the stance-annotated tweet dataset.", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Additional experiments are conducted, including error analysis and investigation of the limitations of existing works. These experiments include analyzing the quality of irony generation, sentiment preservation, and content preservation in the transformed sentences, as well as comparing the results with other works. The results indicate that our model outperforms other state-of-the-art approaches and provides a good balance between irony accuracy and sentiment preservation.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention replaces the standard self-attention with a variant called Gaussian-masked directional multi-head attention. The model uses the score matrix produced by the query and key-value pairs to generate weights in the attention, ensuring the Gaussian-weight matrix is equal to the probability of each segmentation gap and predicting the word boundaries of words. The model combines the information of two adjacent characters to capture the localization information and directional information, making it suitable for the Transformer-based model.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "social media.", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the base models used in the proposed framework for detecting sarcasm, including the pre-trained CNN-SVM (CNN) and the difference features extracted from the pre-trained CNN-SVM. These features are concatenated to form the final feature vector for classification.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "No.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval 2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average of 19.55 tokens on average, with 156.1 sentences per document, and 167,739 words in total.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "text classification.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Their model is compared to handcrafted rules-based pattern matching methods and/or machine learning approaches.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "These versions of ELMo have been trained on larger training sets, with the original English ELMo model being trained on a larger corpus (100 million tokens) compared to the 20 million tokens used in the original ELMoForManyLangs models.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "64, 16, 20, 22, 24, 25, 64, 16, 30, 64, 128, 256, 512, 1024, 16225, 64, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, MWMOTE, and MCMC.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes. Their proposed MNER model combines text and image features for improved NER performance, achieving state-of-the-art results on the SnapCaptions dataset.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.4325", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on both the Unsupervised POS Induction task and the Unsupervised Dependency Parsing task.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "Yes.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "Yes, our proposed KB relation detection model achieves the state of the art on both the SimpleQuestions BIBREF2 and WebQSP benchmarks. Specifically, our model outperforms the previous methods on both tasks, achieving a 2-3% improvement on SimpleQuestions and a 76.11% improvement on WebQSP. Additionally, our proposed KB relation detection model demonstrates state-of-the-art performance on both tasks, achieving a 93.3% top-1 accuracy on SimpleQuestions and a 80.65% top-1 accuracy on WebQSP. Furthermore, our KB relation detection model out", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
