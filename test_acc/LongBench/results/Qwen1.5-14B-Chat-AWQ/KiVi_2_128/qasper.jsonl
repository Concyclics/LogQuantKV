{"pred": "The ground truth for fake news in the study is established through manual annotation by an expert, who labels the tweets as containing fake news or not based on the text field.", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of NetVLAD, used for language identification, which improves accuracy by adding ghost clusters to handle noisy or irrelevant content.", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "Their model outperforms the state-of-the-art results by 68.8% to 71.8%.", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Additional features include character-level representations, context tweets, and Latent Topic Clustering (LTC) for RNN models. Context tweets are used to provide extra contextual information by considering the tweet's reply or quoted tweet.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "A baseline method and evaluation scripts are provided for the proposed task.", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The datasets used for evaluation are CNN/DailyMail, NYT, and XSum.", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach outperforms existing word similarity and entailment datasets, achieving better correlation and F1-score.", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "Their ensemble method works by starting with the best-performing model and iteratively adding the best-performing untried model until it improves validation performance, discarding it otherwise, until a final ensemble of 5 models is formed.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The sources of the datasets are the Friends TV sitcom scripts and Facebook Messenger chats from EmotionPush.", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The article does not provide a specific accuracy for the proposed system.", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes.", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "Twitter data for finance-related posts and news articles, as well as a set of 246,945 documents for dependency parsing.", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "unanswerable", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "They compared the performance of SMT and various NMT models, including RNN-based NMT and Transformer-based NMT, on the built dataset.", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are: (1) neutral feature regularization, (2) maximum entropy of class distribution regularization, and (3) KL divergence between reference and predicted class distribution regularization.", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines include SVM with unigram, bigram, and trigram features, SVM with average word embeddings, SVM with transformed word embeddings, CNN, RCNN, and UTCNN variations without user, topic, or comment information.", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The improvement in performance is not specified in the article.", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "Their model improves interpretability by introducing sparse attention with adaptive shape and sparsity, leading to clearer identification of head behaviors and increased interpretability.", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline is the context-agnostic translation model, which serves as the input for the DocRepair model and is being repaired or improved by the model.", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The metrics used for evaluation are cross-lingual Natural Language Inference (XNLI) and dependency parsing (Labeled Attachment Scores or LAS).", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on a large-scale MT dataset.", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Stylistic features like emoticons, laughter expressions, and patterns related to situational disparity.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "Jasper architecture, specifically Jasper DR, is a family of end-to-end ASR models that replace acoustic and pronunciation models with a convolutional neural network, achieving state-of-the-art results on LibriSpeech and competitive performance on other datasets.", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "They look at 22,880 users in the train set, 2,500 users in the development set, and 2,500 users in the test set.", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "Metrics used for evaluation include perplexity, user-ranking, BLEU-1/4, ROUGE-L, BLEU-1/4, Distinct-1/2, recipe-level coherence, and step entailment.", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create a \"No Answer\" label for when a patient does not mention a symptom, and they manually verify the groundtruth output for multi-turn dialogue comprehension.", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "unanswerable", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The experiments are conducted on four machine translation tasks: IWSLT 2017 German $\\rightarrow $ English, KFTT Japanese $\\rightarrow $ English, WMT 2016 Romanian $\\rightarrow $ English, and WMT 2014 English $\\rightarrow $ German.", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The article does not provide a specific improvement percentage for Estonian in the NER task.", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are scholars from diverse disciplinary backgrounds with expertise in computational text analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "unanswerable", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages (zul, xho, nbl, ssw) and the Sotho languages (nso, sot, tso) are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared layer-wise trained models with models initialized with Xavier initialization.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "They used 29,794 articles from the Wikipedia dataset, and the arXiv dataset consists of 3 subsets with varying numbers of papers.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "Models are evaluated by measuring the efficiency of the communication scheme (retention rate of tokens) and the accuracy of reconstruction (fraction of sentences generated with exact matches).", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "Precision, Recall, and F-measure.", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain with labeled data, and the target domain is the new domain with either no or very little labeled data.", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "They compare their model with state-of-the-art methods, including RNNs with various gating structures and transformations like LSTMs, gated linear transformations, and convolutional neural networks.", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "Neural network modules in NeuronBlocks include embedding layers, neural network layers (e.g., RNN, CNN, Transformer), attention mechanisms, dropout and batch norm layers, and various loss and metric functions.", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "They used the multilingual pronunciation corpus collected by deri2016grapheme, which consists of spelling–pronunciation pairs extracted from Wiktionary.", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines for the speculation detection and scope resolution tasks are not explicitly stated in the article.", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "Spanish and Finnish", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test the method on the task of predicting hashtags for social media posts, and mention the potential for future work on using the embeddings for tasks like tracking infectious diseases.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They obtain psychological dimensions of people by analyzing the language used in their collected blog posts and utilizing tools like the Linguistic Inquiry and Word Count (LIWC) to identify patterns in word categories such as positive feelings and money use, as well as by examining core values and themes in the blog content.", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify the components of an argument, specifically the components of the Toulmin's model, such as claims, premises, backing, rebuttals, and refutations.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "N-grams of arbitrary length are not mentioned to be aligned using PARENT; instead, PARENT computes precision and recall against both the reference and the table.", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset consists of 1,873 conversation threads, roughly 14k tweets.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered in the Multi-SimLex initiative are English, Mandarin, Russian, French, Spanish, Polish, Estonian, Finnish, Hebrew, Croatian, German, Italian, and a mixture of resource-rich and low-resource languages like Welsh and Kiswahili.", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "model is applied to the Wikipedia dataset and the Reddit CMV dataset.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "Unanswerable.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by checking sentence-level BLEU scores, manual inspection of low-quality translations, perplexity measurements, and using VizSeq for similarity scores.", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They combine the audio-RNN and text-RNN outputs using a feed-forward neural network that takes the concatenation of the audio-RNN's final encoding vector and the text-RNN's hidden state as input.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "Their model improved by 2.11 BLEU, 1.7 FKGL, and 1.07 SARI.", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "700", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets went viral if they were retweeted more than 1000 times.", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "BERT performs best.", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The source of the data is a large-scale speech corpus collected using crowdsourcing.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The article mentions two methods: a deep learning model based on a neural network and Logistic Regression as a feature-based approach using nine features.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the social honeypot dataset collected by Lee et al., and its quality is not explicitly mentioned but it is described as being built by attracting social spammers, suggesting it might be of some quality for the purpose of spammer detection.", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder is an LSTM with an attention mechanism.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "BERT-based models are the best-performing, with an ensemble of Logistic Regression, CNN, and BERT achieving 67.3% F1 score on the external validation set for the sentence-level propaganda detection task, and the ensemble of LSTM-CRF+Multi-task with BERT achieving 63.2% F1 score on the external validation set for the fragment-level propaganda detection task.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline for the multilingual multistage fine-tuning approach is a model trained on a combination of in-domain and out-of-domain data for Ja INLINEFORM0 En and Ru INLINEFORM1 En, achieving a BLEU score for Ja INLINEFORM2 Ru that is lower than the strong baseline established with monolingual data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word2Vec (word2vec), word embeddings, and retrofitting vector method by YuCBJW16 are explored in the paper.", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They use the CFILT-preorder system for reordering English sentences to match the word order of the source language.", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "unanswerable", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Seven experts with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "CNN-RNN for image-to-poem and seq2seq with attention for text style transfer.", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The article does not specify whether the RNN layer or the Transformer layer works better on top of BERT; it only describes the two approaches (RoBERT and ToBERT) that use additional layers after BERT for long sequence classification.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They propose extended middle context, a new context representation for CNNs in relation classification.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Three (3) different types of entities exist in the dataset: Person (PER), Location (LOC), and Organization (ORG).", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data from expert annotators is higher quality than that from crowd annotators.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The imbalance in the analyzed corpora is 65% of men and 33.16% of women, with women speaking less than men and representing only 22.57% of the total speech time.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "Multi30K", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "Previous state-of-the-art models in CWS.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression and Multilayer Perceptron (MLP) are used as target models.", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "They used NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, and BIBREF18 for entity-level sentiment analysis, and BIBREF17, BIBREF19, BIBREF20, BIBREF21, and BIBREF22 for named-entity recognition.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "Experiments are conducted on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "Existing approaches for modeling urban regions include identifying points-of-interest, itineraries, and using Flickr tags for environmental information.", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used three datasets for evaluation: CSAT for CSAT prediction, 20 newsgroups for topic identification, and Fisher Phase 1 corpus for topic identification.", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "unanswerable", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition requires that the neural projector is invertible, with the inverse and Jacobian of the function existing, to enable tractable exact inference and marginal likelihood computation.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema involves annotating dimensions such as problem setting, expected answer type, factual correctness, required reasoning, and linguistic complexity of the questions, answers, and context in MRC gold standards.", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of the WikiSmall dataset is 89,042 sentence pairs, and the size of the WikiLarge dataset is 296,402 sentence pairs.", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "Vanilla ST baseline, encoder pre-training, decoder pre-training, encoder-decoder pre-training, many-to-many pre-training, triangle+pre-train, and a cascaded system with ASR and MT models.", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "SVMs, a bidirectional LSTM model, and a CNN model are used in the experiment.", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " unanswerable", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe BIBREF13 and Edinburgh embeddings BIBREF14, along with emoji vectors BIBREF16, were used.", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "Their personalized models outperform the baselines in terms of perplexity, user-ranking, recipe-level coherence, and step entailment, with the Prior Name model achieving the best results in personalization metrics.", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "A combination of irony reward and sentiment reward.", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The model has limitations due to the lack of an end-to-end dataset, which can result in generated English poems not working well with Shakespeare style transfer for certain paintings.", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared their model to the Affective Text, Fairy Tales, and ISEAR datasets, which are commonly used for emotion classification.", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The results showed differences in the distribution of followers, number of URLs, and verification status of users for tweets containing fake news compared to those not containing fake news.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and is manually curated.", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The article mentions Persian and English accents are present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace represents a low-dimensional linear subspace in a high-dimensional word vector space, capturing the context of a text and its class information.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF) and Support Vector Machines (SVM) are used as baseline models.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "unanswerable", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "unanswerable", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "They used the datasets from the SemEval-2016 “Sentiment Analysis in Twitter” task, specifically the datasets for fine-grained and ternary sentiment classification.", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model, which is smaller than BERT$_\\mathrm {LARGE}$.", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control, but the article mentions that it is challenging to validate the quality at a large scale and that systematic biases can be introduced, highlighting the need for rigorous baselines and validation.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "Their models achieved competitive or state-of-the-art results on some emotion labels for the evaluation datasets.", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed in the article is a novel three-tag system that indicates the position of the pun in the context (before, as the pun, or after the pun).", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The robustness of a model in this context refers to the model's ability to handle prior knowledge effectively and not be overly influenced by biased or incomplete knowledge, making it more practical and stable.", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "InferSent and Universal Sentence Encoder.", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method achieves significant performance improvements on the NER task, with +0.97 F1 score on CoNLL2003 and +0.96 on OntoNotes5.0.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on Quora Duplicate Question Detection and ranking questions in Bing's People Also Ask.", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against previous syntactic tree-based models and latent tree models.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the improved relation detection model, specifically the Hierarchical Residual BiLSTM (HR-BiLSTM) model, which helps with entity re-ranking and core relation detection in the KBQA system.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the Nearest-Neighbor model and the Encoder-Decoder model with ingredient attention (Enc-Dec).", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "Manually inspecting the data, part-of-speech tagging, and leveraging coreference annotations are considered to find examples of biases and unwarranted inferences.", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "French", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with Cell-aware Stacked LSTMs (CAS-LSTM) and compared them to plain stacked LSTMs.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "Sumy package and an ILP-based summarization algorithm (ILP)", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "Previous works have proposed models to aid instructors in selectively intervening on student discussions in MOOC forums, but the state-of-the-art models for predicting instructor intervention were improved upon in this paper.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The component that is least impactful, as mentioned in the article, is the no master node, where removing it leads to a decrease in performance.", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the DTA corpus, containing texts from the 16th to the 20th century in German, with DTA18 and DTA19 being specific subsets from the 1750-1799 and 1850-1899 time periods.", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "Surprisingly, the model has the ability to transfer between low lexical similarity language pairs, such as English and Chinese, even in a zero-shot setting.", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model, ALOHA, outperforms the baselines in recovering the language style of specific characters.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "The article does not provide a specific numerical improvement; it states that ARAML outperforms other baselines on various text generation tasks, but the exact improvement is not quantified.", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The model detects some biases in the process of collecting or annotating datasets, as seen in the error analysis and the observation that the model can differentiate between different types of biases in the data.", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, multiple baselines were tested, including SVM, CNN, and a word count baseline, to compare with the neural baseline.", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The size of the OurNepali dataset is not explicitly mentioned in the article, but it is stated that it is significantly larger than the ILPRL dataset, and the OurNepali dataset is ten times bigger in terms of entities.", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "Replacing training objective with dice loss improves F1 score for paraphrase identification.", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The article mentions a dataset from BIBREF0, which is used for the ERP data analysis.", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "Stimulus-based, imagined, and articulated speech state corresponding to phonemic/syllabic and word categories were presented to the subjects.", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, and Pointer-Gen+ARL-SEN", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "Neural network based models and traditional machine learning classifiers (Naïve Bayes, Logistic Regression, SVM, Random Forests, Gradient Boosted Trees, and Convolutional Neural Networks, Recurrent Neural Networks, and their variants) are used on the dataset.", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "Bi-directional and uni-directional language models with self-attention architecture.", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "Weights are dynamically adjusted using a strategy that associates each training example with a weight proportional to $(1-p)$, where $p$ is the probability, and this weight changes as training proceeds.", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed exploration strategies, specifically KG-A2C-chained and KG-A2C-Explore, significantly outperform the baseline A2C and KG-A2C methods, with KG-A2C-chained being more sample-efficient and KG-A2C-Explore taking longer to reach a similar reward but consistently passing the bottleneck.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a monolingual model with crosslingual latent variables to capture crosslingual semantic role agreements between aligned constituents in parallel languages.", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified through annotations in the resource, including non-verbal articulations and undefined sound or pronunciations.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter RNN (ScRNN) is a type of word recognition model that processes a sentence of words with misspelled characters, predicting the correct words by considering the first and last characters of each word and the internal characters.", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL consistently outperforms other baselines with a favorable generalization ability.", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used for evaluating error detection performance was the error detection system by Rei2016 trained on the same FCE dataset.", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "They obtained the annotated clinical notes from the 2010 i2b2/VA BIBREF0 dataset.", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the model to consider both context information from the source document and the previously generated summary when predicting each word, thus improving the refinement of the summary.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "They use a combination of book corpus and Twitter data for training.", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features.", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with binary labels indicating the presence or absence of depression or specific depressive symptoms in tweets, with each tweet being labeled as either no evidence of depression or evidence with one or more depressive symptoms.", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "They evaluated GreenBioBERT on eight publicly available biomedical NER tasks used in BIBREF2.", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by translating the English datasets into Spanish using the machine translation platform Apertium.", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a multinomial Naive Bayes classifier with varying feature selection methods, including IGR, AFR, and word embeddings for industry prediction.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for the SLC task used a simple logistic regression classifier with a single feature: the length of the sentence. For the FLC task, the baseline generated spans and selected a technique randomly.", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with baselines that do not adopt joint learning, as shown in the first block of Table TABREF11.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "Political bias is included in the US dataset by assigning a political bias label to articles based on the procedure described in BIBREF2, and this is done by referring to the procedure in which different news outlets are labeled as left-biased or right-biased.", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from ancient Chinese articles in different dynasties and articles written by celebrities of that era.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "unanswerable", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three convolutional layers.", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "European network of nature protected sites Natura 2000 dataset, the ScenicOrNot dataset, and various structured datasets such as numerical and categorical features from science and Flickr tags.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The paper uses two datasets for experiments: NUBes-PHI, a corpus of real medical reports in Spanish, and the MEDDOCAN corpus, a synthetic corpus of clinical cases with sensitive information annotations.", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigram features, pragmatic features, stylistic patterns, and patterns related to situational disparity.", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "Coverage and predictive performance, measured by Matthews correlation coefficient (MCC) and average F1 score, are used to evaluate the system's strategy formulation ability and predictive quality.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Experiments are conducted on the transformation from non-ironic sentences to ironic sentences and from ironic sentences to non-ironic sentences.", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional attention adjusts the weight between characters based on their distance, with longer distances receiving lower attention, and combines this with a multi-head attention mechanism to enhance the representation of sentences.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Facebook", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Baseline features are the features extracted from the sarcasm datasets using the baseline CNN architecture, which are used for sarcasm detection.", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters (k) was varied in the experiments.", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "Their system ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg), and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, with an average of 156.1 sentences per document.", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization and sentiment analysis.", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "Previous methods include syntactic patterns, syntactic dependencies, keyword identification, named entity recognizers, and learned methods like CNN and LSTM variants.", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The previous ELMoForManyLangs models were trained on a significantly smaller dataset, with around 20 million tokens, while the ELMo models described in the article were trained on much larger corpora, with sizes ranging from several hundred million to over 280 million tokens.", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The article does not specify the exact number of sentences in the dataset.", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare their approach to MLP (modified for their data representation), Eusboost, and MWMOTE for the tasks of speech/music classification and emotion classification.", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "Their highest MRR score was achieved in the third test batch set.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank for both unsupervised POS tagging and dependency parsing without gold POS tags.", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this by pointing out that about 87.5% of NLP-related jobs require common tasks and that many engineers find it challenging to build models under existing frameworks like TensorFlow, PyTorch, and Keras due to the large overhead of mastering these frameworks.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve the state of the art on the KBQA end task, specifically on the SimpleQuestions and WebQSP datasets.", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
